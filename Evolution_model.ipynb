{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivD1GMJGPCtw"
   },
   "source": [
    "status : Deep - L - Layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mPHVQ83CMBNN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WL3pYvQuME-k"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Error & Updating\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def binary_cross_entropy(a, y):\n",
    "    return -((y * np.log(a)) + ((1 - y) * np.log(1 - a)))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Activation Function\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def tanh(z):\n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def ReLU(z):\n",
    "    return np.where(z >= 0, z, 0)\n",
    "\n",
    "\n",
    "def LeakyReLU(z: float):\n",
    "    return np.where(z >= 0, z, 0.01 * z)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Derivative of Activation Function wrp. Z\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def dReLU(z: float):\n",
    "    return np.where(z >= 0, 1, 0)\n",
    "\n",
    "\n",
    "def dLeakyReLU(z: float):\n",
    "    return np.where(z >= 0, 1, 0.01)\n",
    "\n",
    "\n",
    "def dTanh(z: float):\n",
    "    a = tanh(z)\n",
    "    return 1 - a ** 2\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For Bi-Deep L layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def thresholder(A, thr):\n",
    "    return np.where(A >= thr, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Sqh2b6lWMFXx"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initiate parameter\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def initiate_param(hyperparam: list):\n",
    "    \"\"\"\n",
    "    Initiate the paramaters W, B for each layer\n",
    "    input : hyperparameter dictionary (specifically, we need the number of unit for every layer)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    n_unit = hyperparam[\"n_unit\"]\n",
    "    L = len(n_unit) - 1\n",
    "    param = dict()\n",
    "\n",
    "    for l in range(L):\n",
    "        param[\"W\" + str(l + 1)] = (\n",
    "            np.random.random(size=(n_unit[l + 1], n_unit[l])) * 0.01\n",
    "        )\n",
    "        param[\"b\" + str(l + 1)] = np.random.random(size=(n_unit[l + 1], 1))\n",
    "\n",
    "    return param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hCAaYiE0MIfs"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Forward Propagation Unit\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def linear_forward(A_prev, W, b):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    linear_cache = (\n",
    "        A_prev,\n",
    "        W,\n",
    "        b,\n",
    "    )  # A :for dZ, W for dA & to get updating, b for updating , dA for dZ\n",
    "    return Z, linear_cache\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation_function):\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "\n",
    "    if activation_function == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    elif activation_function == \"tanh\":\n",
    "        A = tanh(Z)\n",
    "    elif activation_function == \"ReLU\":\n",
    "        A = ReLU(Z)\n",
    "    elif activation_function == \"LeakyReLU\":\n",
    "        A = LeakyReLU(Z)\n",
    "\n",
    "    cache = (Z, linear_cache)  # (Z,A_prev,W,b)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def L_model_forward(X, param, activation_function=\"ReLU\"):\n",
    "    \"\"\"\n",
    "    Forward propagation unit from input to output layer\n",
    "    Argument\n",
    "    1. X --- Input denoted as A[0]\n",
    "    2. param --- Weight and Bias of every layer\n",
    "    3. activation_function --- the activation function of hidden layers (default:ReLU)\n",
    "\n",
    "    Return\n",
    "    1. A --- Output A[L] from the propagation (Z[L] with sigmoid activation function)\n",
    "    2. caches --- the cache of every layer l ; [Z[l] , A[l-1], W[l], b[l]]\n",
    "              --- L elements , each elements (array forms with 4 sub-arrays) contain the cache of its layer\n",
    "                  linear_cache : Z[l]\n",
    "                  activation_cache : A[l-1], W[l], b[l]\n",
    "    \"\"\"\n",
    "\n",
    "    A = X\n",
    "    L = (\n",
    "        len(param) // 2\n",
    "    )  # param stores the weight and bias for L layer, hence len(param) = 2L\n",
    "\n",
    "    caches = []\n",
    "\n",
    "    for l in range(L):  # l = 0,1,2,..,L-1\n",
    "        A_prev = A\n",
    "        W = param[\"W\" + str(l + 1)]\n",
    "        b = param[\"b\" + str(l + 1)]\n",
    "\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, activation_function)\n",
    "        caches.append(cache)  # append cache at layer l+1\n",
    "\n",
    "    return A, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CrtGzWENMMh1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Backward Propagation Unit\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Use dZ from the layer l to obtain dW,dB,dA_prev\n",
    "    Arguments:\n",
    "      dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "      cache -- tuple of values (Z,(A_prev, W, b)) coming from the forward propagation in the current layer (We use only linear cache anyway)\n",
    "\n",
    "    Returns:\n",
    "      dA_prev --- Gradient of the cost with respect to the activation node at the previous layer\n",
    "      dW --- Gradient of the cost with the weight in this layer\n",
    "      db --- Gradient of the cost with the bias in this layer\n",
    "    \"\"\"\n",
    "    _, activation_cache = cache  # We use only activation cache\n",
    "    (a_prev, W, _) = activation_cache  # We do not use b to update\n",
    "    m = dZ.shape[1]\n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, a_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation_function):\n",
    "    \"\"\"\n",
    "    Input dA to find dZ, then use dZ to obtain dW,dB,dA_prev\n",
    "    \"\"\"\n",
    "    if activation_function == \"ReLU\":\n",
    "        g_ = dReLU\n",
    "    elif activation_function == \"LeakyReLU\":\n",
    "        g_ = dLeakyReLU\n",
    "    elif activation_function == \"tanh\":\n",
    "        g_ = dTanh\n",
    "    else:\n",
    "        print(\n",
    "            f\"The activation function {activation_function} not found, ReLU as default\"\n",
    "        )\n",
    "        g_ = dReLU\n",
    "\n",
    "    linear_cache, _ = cache  # We use only linear output cache\n",
    "    Z = linear_cache\n",
    "\n",
    "    dZ = dA * g_(Z)\n",
    "    dA_prev, dW, db = linear_backward(dZ, cache)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def L_model_backward(A, Y, cache, activation_function=\"ReLU\"):\n",
    "    \"\"\"\n",
    "    Do the whole backward propagation model by retrieving the output from forward propagation model and the actual output\n",
    "    Arguments:\n",
    "    A --- A at the layer L\n",
    "    y --- an actual output\n",
    "    cache --- cache from the forward propagation\n",
    "    activation_function --- activation function for the hidden layer\n",
    "    Return:\n",
    "     grads  -- A dictionary with the gradients\n",
    "               grads[\"dA\" + str(l)] = ...\n",
    "               grads[\"dW\" + str(l)] = ...\n",
    "               grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    L = len(cache)  # cache for each layer\n",
    "    grads = {}\n",
    "    dA = np.divide(1 - Y, 1 - A) - np.divide(\n",
    "        Y, A\n",
    "    )  # dA_[L] : Input for the first linear activation backward\n",
    "\n",
    "    for l in reversed(range(L)):\n",
    "\n",
    "        current_cache = cache[l]\n",
    "        (linear_cache, activation_cache) = current_cache\n",
    "        Z = linear_cache\n",
    "        a_prev, W, b = activation_cache  # Start with Z_[L] , A_[L-1], W_[L], b_[L]\n",
    "\n",
    "        dA_prev, dW, db = linear_activation_backward(\n",
    "            dA, current_cache, activation_function\n",
    "        )\n",
    "\n",
    "        grads[\"dW\" + str(l + 1)] = dW\n",
    "        grads[\"db\" + str(l + 1)] = db\n",
    "\n",
    "        dA = dA_prev\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-6NkNadCMRfa"
   },
   "outputs": [],
   "source": [
    "def update_param(param, grads, lr=1e-4):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    1. param -- The current parameter (W1,W2,...,WL,b1,b2,...bL)\n",
    "    2. grads -- the dictionary of gradient that was obtained from L_model_backward function\n",
    "    3. lr (default=1e-4) : Learning rate\n",
    "    Returns:\n",
    "    1. updated_param -- The parameter that got updated\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(param) // 2  # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        param[\"W\" + str(l + 1)] = (\n",
    "            param[\"W\" + str(l + 1)] - lr * grads[\"dW\" + str(l + 1)]\n",
    "        )\n",
    "        param[\"b\" + str(l + 1)] = (\n",
    "            param[\"b\" + str(l + 1)] - lr * grads[\"db\" + str(l + 1)]\n",
    "        )\n",
    "\n",
    "    return param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MZtJnWszMTkj"
   },
   "outputs": [],
   "source": [
    "def cost_reporter(A, Y, m, epoch, Epoch):\n",
    "    \"\"\"\n",
    "    report the cost for each epoch\n",
    "    cost function : Binary cross entropy\n",
    "    Arguments:\n",
    "    A --- predicted value from L-Forward model\n",
    "    y --- actual output\n",
    "    m --- total observations\n",
    "    epoch --- current epoch\n",
    "    Epoch --- total epoch\n",
    "    \"\"\"\n",
    "    loss = binary_cross_entropy(A, Y)\n",
    "    cost = np.divide(\n",
    "        loss, m\n",
    "    )  # No significant difference in speed when compare to '/' though\n",
    "    cost = np.sum(cost, axis=1)\n",
    "    print(f\"Epoch {epoch}/{Epoch} : ===Cost=== : {np.squeeze(cost)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_vVbNcrGMV1v"
   },
   "outputs": [],
   "source": [
    "class Binary_Deep_L_Layer:\n",
    "    \"\"\"\n",
    "    A Deep neural network with L layers\n",
    "    - Able to fit with the predictors (X) and the response (Y)\n",
    "    - Able to predict_proba and predict with threshold\n",
    "    To see the last fit model parameter, uses self.param where self refer to the fit model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hyperparam: dict):\n",
    "        \"\"\"\n",
    "        Launch the Deep_L_layer with the given hyperparameter\n",
    "        Arguments:\n",
    "        hyperparam: A dictionary with key:\n",
    "         L --- Number of Layers (Hidden layer(s) + Output layer)\n",
    "         n_unit --- Number of units of that L layer\n",
    "         lr --- Learning rate\n",
    "         forward_activation_function --- Activation function for all hidden layer(s) in forward model (ReLU,LeakyReLU,tanh,sigmoid)\n",
    "         backward_activation_function --- Activation function for all hidden layer(s) in backward model (ReLU,LeakyReLU,tanh,sigmoid)\n",
    "          {\"L\" : 5,\n",
    "          \"n_unit\" : [nrow,8,6,4,2,1],\n",
    "          \"lr\" : 1e-5,\n",
    "          \"forward_activation_function\" : 'tanh' ,\n",
    "          \"backward_activation_function\" : 'ReLU' }\n",
    "        Supported activation\n",
    "        \"\"\"\n",
    "        self.hyperparam = hyperparam  # assume include nrow in dict\n",
    "\n",
    "        # Explicit hyperparameter attributes\n",
    "        self.L = hyperparam[\"L\"]\n",
    "        self.lr = hyperparam[\"lr\"]\n",
    "        self.forward_activation_function = hyperparam[\"forward_activation_function\"]\n",
    "        self.backward_activation_function = hyperparam[\"backward_activation_function\"]\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        Y: pd.Series,\n",
    "        Epochs: int = 1000,\n",
    "        verbose: bool = True,\n",
    "        new_fit: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fit the launched Deep L layer with the given data X , Y\n",
    "\n",
    "        Arguments:\n",
    "         X --- Pandas Dataframe of predictors\n",
    "         Y --- Pandas Series of response (0 : negative, 1:positive)\n",
    "         Epoch --- number of epochs (default : 1000)\n",
    "         verbose --- report the epochs every 1000 epoch\n",
    "         new_fit --- reset parameters and generate parameters\n",
    "        \"\"\"\n",
    "\n",
    "        ## First, we initiate the attributes\n",
    "\n",
    "        # We turn Dataframe into Numpy format\n",
    "        X = X.to_numpy().T\n",
    "        Y = Y.to_numpy().T\n",
    "        nrow = np.shape(X)[0]\n",
    "\n",
    "        # Assign class attribute\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.m = Y.shape[1]\n",
    "        self.Epochs = Epochs\n",
    "\n",
    "        ## Second, we fit\n",
    "        if new_fit:\n",
    "            self.param = initiate_param(self.hyperparam)\n",
    "\n",
    "        for epoch in range(self.Epochs):\n",
    "            A, cache = L_model_forward(\n",
    "                self.X, self.param, activation_function=self.forward_activation_function\n",
    "            )\n",
    "\n",
    "            if verbose and epoch % 1000 == 0:\n",
    "                cost_reporter(A, self.Y, self.m, epoch, self.Epochs)\n",
    "\n",
    "            grads = L_model_backward(\n",
    "                A, self.Y, cache, self.backward_activation_function\n",
    "            )\n",
    "            self.param = update_param(self.param, grads, lr=self.lr)\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Predict probability of the observation given input X\n",
    "\n",
    "        Arguments:\n",
    "         X --- Pandas Dataframe or Series of predictors\n",
    "        \"\"\"\n",
    "        X = X.to_numpy().T\n",
    "\n",
    "        A_prob, _ = L_model_forward(\n",
    "            X, self.param, activation_function=self.forward_activation_function\n",
    "        )\n",
    "\n",
    "        return A_prob\n",
    "\n",
    "    def predict(self, X, threshold: float = 0.5):\n",
    "        \"\"\"\n",
    "        Predict the observation given input X\n",
    "\n",
    "        Arguments:\n",
    "         X --- Pandas Dataframe or Series of predictors\n",
    "        \"\"\"\n",
    "        A_prob = self.predict_proba(X)\n",
    "        A_pred = thresholder(A_prob, threshold)\n",
    "        return A_pred\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Deep_L_Layer({self.hyperparam})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"A Deep {self.L} Neural network with learning rate = {self.lr} (Forward activation :{self.forward_activation_function},Backward activation :{self.backward_activation_function})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "806lCdjxVXKL"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNaqbnp7VaTC"
   },
   "source": [
    "Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PZKVSAK8RN-5"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/churn_small3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "huthI9Yu4xtz"
   },
   "outputs": [],
   "source": [
    "hyperparam = {\"L\" : 5,\n",
    "              \"n_unit\" : [3,8,6,4,2,1],\n",
    "              \"lr\" : 1e-5,\n",
    "              \"forward_activation_function\" : 'ReLU',\n",
    "              \"backward_activation_function\" : 'ReLU',\n",
    "              \"keep_prob_sequence\" : [1,0.5,0.6,0.7,1,1]}   #Dropout => No dropout = None OR [1,1,1,1,1,1]\n",
    "\n",
    "X = df[['tenure'\t,'TotalCharges'\t,'PaperlessBilling']]\n",
    "Y = df[['Churn']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yakoO0dsNDwB"
   },
   "outputs": [],
   "source": [
    "model = Binary_Deep_L_Layer(hyperparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "YYsJGaQRNdb6",
    "outputId": "70747cae-afe7-49eb-c7a4-3c57cf84b58c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100000 : ===Cost=== : 1.3092259528752233\n",
      "Epoch 1000/100000 : ===Cost=== : 1.18264973341166\n",
      "Epoch 2000/100000 : ===Cost=== : 1.0948250491187088\n",
      "Epoch 3000/100000 : ===Cost=== : 1.028352257196818\n",
      "Epoch 4000/100000 : ===Cost=== : 0.975404404481697\n",
      "Epoch 5000/100000 : ===Cost=== : 0.9317897592718797\n",
      "Epoch 6000/100000 : ===Cost=== : 0.8950027214519832\n",
      "Epoch 7000/100000 : ===Cost=== : 0.8634269049392975\n",
      "Epoch 8000/100000 : ===Cost=== : 0.8359591953306217\n",
      "Epoch 9000/100000 : ===Cost=== : 0.8118133843523231\n",
      "Epoch 10000/100000 : ===Cost=== : 0.7904094270969739\n",
      "Epoch 11000/100000 : ===Cost=== : 0.771307123172449\n",
      "Epoch 12000/100000 : ===Cost=== : 0.7541644330800141\n",
      "Epoch 13000/100000 : ===Cost=== : 0.7387102133851958\n",
      "Epoch 14000/100000 : ===Cost=== : 0.7247257768235769\n",
      "Epoch 15000/100000 : ===Cost=== : 0.7120320610497941\n",
      "Epoch 16000/100000 : ===Cost=== : 0.7004804786881602\n",
      "Epoch 17000/100000 : ===Cost=== : 0.6899462520390899\n",
      "Epoch 18000/100000 : ===Cost=== : 0.6803234662443098\n",
      "Epoch 19000/100000 : ===Cost=== : 0.6715213369073623\n",
      "Epoch 20000/100000 : ===Cost=== : 0.6634613526293585\n",
      "Epoch 21000/100000 : ===Cost=== : 0.6560750587977222\n",
      "Epoch 22000/100000 : ===Cost=== : 0.6493023187251706\n",
      "Epoch 23000/100000 : ===Cost=== : 0.6430899351638719\n",
      "Epoch 24000/100000 : ===Cost=== : 0.6373905473856988\n",
      "Epoch 25000/100000 : ===Cost=== : 0.6321617414447893\n",
      "Epoch 26000/100000 : ===Cost=== : 0.6273653271166963\n",
      "Epoch 27000/100000 : ===Cost=== : 0.6229667464108276\n",
      "Epoch 28000/100000 : ===Cost=== : 0.6189345868485429\n",
      "Epoch 29000/100000 : ===Cost=== : 0.6152401788078541\n",
      "Epoch 30000/100000 : ===Cost=== : 0.6118572607848854\n",
      "Epoch 31000/100000 : ===Cost=== : 0.6087616998465228\n",
      "Epoch 32000/100000 : ===Cost=== : 0.6059312571528643\n",
      "Epoch 33000/100000 : ===Cost=== : 0.6033453904286035\n",
      "Epoch 34000/100000 : ===Cost=== : 0.6009850868147116\n",
      "Epoch 35000/100000 : ===Cost=== : 0.598832720748455\n",
      "Epoch 36000/100000 : ===Cost=== : 0.5968719324833205\n",
      "Epoch 37000/100000 : ===Cost=== : 0.5950875236315605\n",
      "Epoch 38000/100000 : ===Cost=== : 0.5934653667358494\n",
      "Epoch 39000/100000 : ===Cost=== : 0.5919923263865223\n",
      "Epoch 40000/100000 : ===Cost=== : 0.5906561898220581\n",
      "Epoch 41000/100000 : ===Cost=== : 0.5894456053016384\n",
      "Epoch 42000/100000 : ===Cost=== : 0.5883500268338171\n",
      "Epoch 43000/100000 : ===Cost=== : 0.5873596640950935\n",
      "Epoch 44000/100000 : ===Cost=== : 0.5864654365844566\n",
      "Epoch 45000/100000 : ===Cost=== : 0.5856589312407319\n",
      "Epoch 46000/100000 : ===Cost=== : 0.5849323629034411\n",
      "Epoch 47000/100000 : ===Cost=== : 0.58427853712843\n",
      "Epoch 48000/100000 : ===Cost=== : 0.5836908149795929\n",
      "Epoch 49000/100000 : ===Cost=== : 0.5831630795099952\n",
      "Epoch 50000/100000 : ===Cost=== : 0.5826897037215817\n",
      "Epoch 51000/100000 : ===Cost=== : 0.5822655198542437\n",
      "Epoch 52000/100000 : ===Cost=== : 0.5818857899039889\n",
      "Epoch 53000/100000 : ===Cost=== : 0.5815461773078348\n",
      "Epoch 54000/100000 : ===Cost=== : 0.5812427197612879\n",
      "Epoch 55000/100000 : ===Cost=== : 0.5809718031543141\n",
      "Epoch 56000/100000 : ===Cost=== : 0.5807301366248112\n",
      "Epoch 57000/100000 : ===Cost=== : 0.5805147287360461\n",
      "Epoch 58000/100000 : ===Cost=== : 0.5803228647874129\n",
      "Epoch 59000/100000 : ===Cost=== : 0.5801520852672684\n",
      "Epoch 60000/100000 : ===Cost=== : 0.5800001654534115\n",
      "Epoch 61000/100000 : ===Cost=== : 0.5798650961617895\n",
      "Epoch 62000/100000 : ===Cost=== : 0.579745065637933\n",
      "Epoch 63000/100000 : ===Cost=== : 0.5796384425790015\n",
      "Epoch 64000/100000 : ===Cost=== : 0.5795437602676052\n",
      "Epoch 65000/100000 : ===Cost=== : 0.5794597017921321\n",
      "Epoch 66000/100000 : ===Cost=== : 0.5793850863223884\n",
      "Epoch 67000/100000 : ===Cost=== : 0.5793188564041434\n",
      "Epoch 68000/100000 : ===Cost=== : 0.5792600662317668\n",
      "Epoch 69000/100000 : ===Cost=== : 0.5792078708546082\n",
      "Epoch 70000/100000 : ===Cost=== : 0.5791615162700908\n",
      "Epoch 71000/100000 : ===Cost=== : 0.5791203303546693\n",
      "Epoch 72000/100000 : ===Cost=== : 0.5790837145827521\n",
      "Epoch 73000/100000 : ===Cost=== : 0.5790511364833579\n",
      "Epoch 74000/100000 : ===Cost=== : 0.5790221227845794\n",
      "Epoch 75000/100000 : ===Cost=== : 0.5789962531967636\n",
      "Epoch 76000/100000 : ===Cost=== : 0.5789731547866204\n",
      "Epoch 77000/100000 : ===Cost=== : 0.5789524968961284\n",
      "Epoch 78000/100000 : ===Cost=== : 0.5789339865620612\n",
      "Epoch 79000/100000 : ===Cost=== : 0.5789173643941194\n",
      "Epoch 80000/100000 : ===Cost=== : 0.5789024008719604\n",
      "Epoch 81000/100000 : ===Cost=== : 0.5788888930238201\n",
      "Epoch 82000/100000 : ===Cost=== : 0.5788766614518546\n",
      "Epoch 83000/100000 : ===Cost=== : 0.5788655476717632\n",
      "Epoch 84000/100000 : ===Cost=== : 0.5788554117366538\n",
      "Epoch 85000/100000 : ===Cost=== : 0.5788461301174352\n",
      "Epoch 86000/100000 : ===Cost=== : 0.5788375938142769\n",
      "Epoch 87000/100000 : ===Cost=== : 0.5788297066758108\n",
      "Epoch 88000/100000 : ===Cost=== : 0.5788223839047855\n",
      "Epoch 89000/100000 : ===Cost=== : 0.5788155507307902\n",
      "Epoch 90000/100000 : ===Cost=== : 0.5788091412324584\n",
      "Epoch 91000/100000 : ===Cost=== : 0.5788030972932154\n",
      "Epoch 92000/100000 : ===Cost=== : 0.578797367676178\n",
      "Epoch 93000/100000 : ===Cost=== : 0.5787919072052321\n",
      "Epoch 94000/100000 : ===Cost=== : 0.5787866760406137\n",
      "Epoch 95000/100000 : ===Cost=== : 0.5787816390385103\n",
      "Epoch 96000/100000 : ===Cost=== : 0.5787767651852874\n",
      "Epoch 97000/100000 : ===Cost=== : 0.5787720270979277\n",
      "Epoch 98000/100000 : ===Cost=== : 0.5787674005831676\n",
      "Epoch 99000/100000 : ===Cost=== : 0.5787628642486251\n"
     ]
    }
   ],
   "source": [
    "model.fit(X,Y,Epochs=100000)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Evolution model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
