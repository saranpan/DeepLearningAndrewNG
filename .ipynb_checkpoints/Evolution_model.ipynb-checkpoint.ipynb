{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivD1GMJGPCtw"
   },
   "source": [
    "status : Add Weight Init to Deep - L - Layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "mPHVQ83CMBNN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "WL3pYvQuME-k"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Error & Updating\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def binary_cross_entropy(a, y):\n",
    "    return -((y * np.log(a)) + ((1 - y) * np.log(1 - a)))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Activation Function\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def tanh(z):\n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def ReLU(z):\n",
    "    return np.where(z >= 0, z, 0)\n",
    "\n",
    "\n",
    "def LeakyReLU(z: float):\n",
    "    return np.where(z >= 0, z, 0.01 * z)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Derivative of Activation Function wrp. Z\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def dReLU(z: float):\n",
    "    return np.where(z >= 0, 1, 0)\n",
    "\n",
    "\n",
    "def dLeakyReLU(z: float):\n",
    "    return np.where(z >= 0, 1, 0.01)\n",
    "\n",
    "def dsigmoid(z: float):\n",
    "    a = sigmoid(z)\n",
    "    return a*(1 - a)\n",
    "\n",
    "def dTanh(z: float):\n",
    "    a = tanh(z)\n",
    "    return 1 - a ** 2\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For Bi-Deep L layer Classification\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def thresholder(A, thr):\n",
    "    return np.where(A >= thr, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dec(parameters, X):\n",
    "    \"\"\"\n",
    "    Used for plotting decision boundary.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (m, K)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Predict using forward propagation and a classification threshold of 0.5\n",
    "    a3, cache = L_model_forward(X, parameters)\n",
    "    predictions = (a3>0.5)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "Sqh2b6lWMFXx"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initiate parameter\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def initiate_param(hyperparam: list,initialization = 'random',seed:int=42):\n",
    "    \"\"\"\n",
    "    Initiate the paramaters W, B for each layer\n",
    "    input : hyperparameter dictionary (specifically, we need the number of unit for every layer)\n",
    "    \"\"\"\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    n_unit = hyperparam[\"n_unit\"]\n",
    "\n",
    "    if initialization == 'zero':\n",
    "        param = initialization_zero(n_unit)\n",
    "    elif initialization == 'random':\n",
    "        param = initialization_random(n_unit)\n",
    "    elif initialization == 'He':\n",
    "        param = initialization_he(n_unit)\n",
    "    elif initialization == 'Xavier':    \n",
    "        param = initialization_xavier(n_unit)\n",
    "    else: #default : random\n",
    "        print(f'''There is no weight initialization called \"{initialization}\"\n",
    "              switch to default initialization random\n",
    "              ''')\n",
    "        param = initialization_random(n_unit)\n",
    "        \n",
    "    return param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_zero(n_unit:list):\n",
    "    \"\"\"\n",
    "    Initialize both weight and bias as zeros\n",
    "    \n",
    "    Argument:\n",
    "    n_units : number of units for every layer respectively\n",
    "    \n",
    "    Returns:\n",
    "    param : parameter W, B for every layer from 1 to L\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(n_unit) - 1  #Exclude input layer to calculating L\n",
    "    param = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        param[\"W\" + str(l)] = np.zeros(shape=(n_unit[l], n_unit[l-1])) * 0.01 # Uniform(0,1] * 0.01\n",
    "        param[\"b\" + str(l)] = np.ones(shape=(n_unit[l], 1))\n",
    "    return param\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_random(n_unit:list,scale:int=0.01):\n",
    "    \"\"\"\n",
    "    Initialize weight randomly with Normal(mean=0,sigma=1)\n",
    "    Initialize bias as zeros\n",
    "    \n",
    "    Argument:\n",
    "    n_units : number of units for every layer respectively\n",
    "    \n",
    "    Returns:\n",
    "    param : parameter W, B for every layer from 1 to L\n",
    "    \"\"\"\n",
    "    L = len(n_unit) - 1  #Exclude input layer to calculating L\n",
    "    param = {}\n",
    "    \n",
    "    \"\"\"\n",
    "    scale : variance of the random variable\n",
    "    y = scale * x\n",
    "    var(y) = var(scale*x)\n",
    "    var(y) = scale^2 * x\n",
    "    \"\"\"\n",
    "    for l in range(1,L+1):\n",
    "        param[\"W\" + str(l)] = np.random.randn(n_unit[l], n_unit[l-1]) * scale # Normal(0,1) * scale \n",
    "        param[\"b\" + str(l)] = np.random.rand(n_unit[l], 1)\n",
    "    return param\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_he(n_unit:list):\n",
    "    \"\"\"\n",
    "    Initialize weight randomly with Normal(mean=0,sigma=(2/fan_in))\n",
    "    Initialize bias as zeros\n",
    "    \n",
    "    Argument:\n",
    "    n_units : number of units for every layer respectively\n",
    "    \n",
    "    Returns:\n",
    "    param : parameter W, B for every layer from 1 to L\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(n_unit) - 1  #Exclude input layer to calculating L\n",
    "    param = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        fan_in = n_unit[l-1]\n",
    "        \n",
    "        param[\"W\" + str(l)] = np.random.randn(n_unit[l], n_unit[l-1]) * np.sqrt(2/fan_in) \n",
    "        param[\"b\" + str(l)] =  np.random.rand(n_unit[l], 1)\n",
    "    return param\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_xavier(n_unit:list):\n",
    "    \"\"\"\n",
    "    Initialize weight randomly with Normal(mean=0,sigma=(1/fan_avg))\n",
    "    Initialize bias as zeros\n",
    "    \n",
    "    Argument:\n",
    "    n_units : number of units for every layer respectively\n",
    "    \n",
    "    Returns:\n",
    "    param : parameter W, B for every layer from 1 to L\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(n_unit) - 1  #Exclude input layer to calculating L\n",
    "    param = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        fan_in , fan_out = n_unit[l-1] , n_unit[l]\n",
    "        fan_avg = 1/2 * (fan_in + fan_out)\n",
    "        \n",
    "        param[\"W\" + str(l)] = np.random.randn(n_unit[l], n_unit[l-1]) * np.sqrt(1/fan_avg) \n",
    "        param[\"b\" + str(l)] =  np.random.rand(n_unit[l], 1)\n",
    "    return param\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "hCAaYiE0MIfs"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Forward Propagation Unit\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def linear_forward(A_prev, W, b):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    linear_cache = (A_prev,W,b)  # A :for dZ, W for dA & to get updating, b for updating , dA for dZ\n",
    "    return Z, linear_cache\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation_function):\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "\n",
    "    if activation_function == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    elif activation_function == \"tanh\":\n",
    "        A = tanh(Z)\n",
    "    elif activation_function == \"ReLU\":\n",
    "        A = ReLU(Z)\n",
    "    elif activation_function == \"LeakyReLU\":\n",
    "        A = LeakyReLU(Z)\n",
    "\n",
    "    cache = (Z, linear_cache)  # (Z,A_prev,W,b)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def L_model_forward(X, param, activation_function=\"ReLU\"):\n",
    "    \"\"\"\n",
    "    Forward propagation unit from input to output layer\n",
    "    Argument\n",
    "    1. X --- Input denoted as A[0]\n",
    "    2. param --- Weight and Bias of every layer\n",
    "    3. activation_function --- the activation function of hidden layers (default:ReLU)\n",
    "\n",
    "    Return\n",
    "    1. A --- Output A[L] from the propagation (Z[L] with sigmoid activation function)\n",
    "    2. caches --- the cache of every layer l ; [Z[l] , A[l-1], W[l], b[l]]\n",
    "              --- L elements , each elements (array forms with 4 sub-arrays) contain the cache of its layer\n",
    "                  linear_cache : Z[l]\n",
    "                  activation_cache : A[l-1], W[l], b[l]\n",
    "    \"\"\"\n",
    "\n",
    "    A = X\n",
    "    L = (len(param) // 2)  # param stores the weight and bias for L layer, hence len(param) = 2L\n",
    "\n",
    "    caches = []\n",
    "\n",
    "    for l in range(1,L+1):  # l = 1,2,..,L-1\n",
    "        A_prev = A\n",
    "        W = param[\"W\" + str(l)]\n",
    "        b = param[\"b\" + str(l)]\n",
    "        \n",
    "        if l == L:  \n",
    "            activation_function = 'sigmoid'\n",
    "            \n",
    "        A, cache = linear_activation_forward(A_prev, W, b, activation_function)\n",
    "        caches.append(cache)  # append cache at layer l\n",
    "\n",
    "    return A, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "CrtGzWENMMh1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Backward Propagation Unit\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Use dZ from the layer l to obtain dW,dB,dA_prev\n",
    "    Arguments:\n",
    "      dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "      cache -- tuple of values (Z,(A_prev, W, b)) coming from the forward propagation in the current layer (We use only linear cache anyway)\n",
    "\n",
    "    Returns:\n",
    "      dA_prev --- Gradient of the cost with respect to the activation node at the previous layer\n",
    "      dW --- Gradient of the cost with the weight in this layer\n",
    "      db --- Gradient of the cost with the bias in this layer\n",
    "    \"\"\"\n",
    "    _, activation_cache = cache  # We use only activation cache\n",
    "    (a_prev, W, _) = activation_cache  # We do not use b to update\n",
    "    m = dZ.shape[1]\n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, a_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation_function):\n",
    "    \"\"\"\n",
    "    Input dA to find dZ, then use dZ to obtain dW,dB,dA_prev\n",
    "    \"\"\"\n",
    "    if activation_function == \"ReLU\":\n",
    "        g_ = dReLU\n",
    "    elif activation_function == \"LeakyReLU\":\n",
    "        g_ = dLeakyReLU\n",
    "    elif activation_function == \"tanh\":\n",
    "        g_ = dTanh\n",
    "    elif activation_function == \"sigmoid\":\n",
    "        g_ = dsigmoid\n",
    "    else:\n",
    "        print(f\"The activation function {activation_function} not found, ReLU as default\")\n",
    "        g_ = dReLU\n",
    "\n",
    "    linear_cache, _ = cache  # We use only linear output cache\n",
    "    Z = linear_cache\n",
    "\n",
    "    dZ = dA * g_(Z)\n",
    "    dA_prev, dW, db = linear_backward(dZ, cache)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def L_model_backward(A, Y, cache, activation_function=\"ReLU\"):\n",
    "    \"\"\"\n",
    "    Do the whole backward propagation model by retrieving the output from forward propagation model and the actual output\n",
    "    Arguments:\n",
    "    A --- A at the layer L\n",
    "    y --- an actual output\n",
    "    cache --- cache from the forward propagation\n",
    "    activation_function --- activation function for the hidden layer\n",
    "    Return:\n",
    "     grads  -- A dictionary with the gradients\n",
    "               grads[\"dA\" + str(l)] = ...\n",
    "               grads[\"dW\" + str(l)] = ...\n",
    "               grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    L = len(cache)  # cache for each layer\n",
    "    grads = {}\n",
    "    dA = np.divide(1 - Y, 1 - A) - np.divide(Y, A)  # dA_[L] : Input for the first linear activation backward\n",
    "                                                    # Loss : Binary Cross Entropy\n",
    "\n",
    "    for l in reversed(range(1,L+1)):\n",
    "\n",
    "        current_cache = cache[l-1] #Fix\n",
    "        (linear_cache, activation_cache) = current_cache\n",
    "        Z = linear_cache\n",
    "        a_prev, W, b = activation_cache  # Start with Z_[L] , A_[L-1], W_[L], b_[L]\n",
    "        \n",
    "        if l == L: # Apply dsigmoid for last layer only\n",
    "            dA_prev, dW, db = linear_activation_backward(dA, current_cache, activation_function='sigmoid') \n",
    "            \n",
    "        dA_prev, dW, db = linear_activation_backward(dA, current_cache, activation_function)\n",
    "\n",
    "        grads[\"dW\" + str(l)] = dW\n",
    "        grads[\"db\" + str(l)] = db\n",
    "\n",
    "        dA = dA_prev\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "-6NkNadCMRfa"
   },
   "outputs": [],
   "source": [
    "def update_param(param, grads, lr=1e-4):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    1. param -- The current parameter (W1,W2,...,WL,b1,b2,...bL)\n",
    "    2. grads -- the dictionary of gradient that was obtained from L_model_backward function\n",
    "    3. lr (default=1e-4) : Learning rate\n",
    "    Returns:\n",
    "    1. updated_param -- The parameter that got updated\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(param) // 2  # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(1,L+1):\n",
    "        param[\"W\" + str(l)] = (param[\"W\" + str(l)] - lr * grads[\"dW\" + str(l)])\n",
    "        param[\"b\" + str(l)] = (param[\"b\" + str(l)] - lr * grads[\"db\" + str(l)])\n",
    "\n",
    "    return param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "id": "MZtJnWszMTkj"
   },
   "outputs": [],
   "source": [
    "def cost_reporter(A, Y, m, epoch, Epoch):\n",
    "    \"\"\"\n",
    "    report the cost for each epoch\n",
    "    cost function : Binary cross entropy\n",
    "    Arguments:\n",
    "    A --- predicted value from L-Forward model\n",
    "    y --- actual output\n",
    "    m --- total observations\n",
    "    epoch --- current epoch\n",
    "    Epoch --- total epoch\n",
    "    \"\"\"\n",
    "    loss = binary_cross_entropy(A, Y)\n",
    "    cost = np.divide(\n",
    "        loss, m\n",
    "    )  # No significant difference in speed when compare to '/' though\n",
    "    cost = np.sum(cost, axis=1)\n",
    "    print(f\"Epoch {epoch}/{Epoch} : ===Cost=== : {np.squeeze(cost)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "id": "_vVbNcrGMV1v",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Binary_Deep_L_Layer:\n",
    "    \"\"\"\n",
    "    A Deep neural network with L layers\n",
    "    - Able to fit with the predictors (X) and the response (Y)\n",
    "    - Able to predict_proba and predict with threshold\n",
    "    To see the last fit model parameter, uses self.param where self refer to the fit model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hyperparam: dict):\n",
    "        \"\"\"\n",
    "        Launch the Deep_L_layer with the given hyperparameter\n",
    "        \n",
    "        Arguments:\n",
    "        hyperparam: A dictionary with key:\n",
    "         L --- Number of Layers (Hidden layer(s) + Output layer)\n",
    "         n_unit --- Number of units of that L layer\n",
    "         lr --- Learning rate\n",
    "         forward_activation_function --- Activation function for all hidden layer(s) in forward model (ReLU,LeakyReLU,tanh,sigmoid)\n",
    "         backward_activation_function --- Activation function for all hidden layer(s) in backward model (ReLU,LeakyReLU,tanh,sigmoid)\n",
    "          {\"L\" : 5,\n",
    "          \"n_unit\" : [nrow,8,6,4,2,1],\n",
    "          \"lr\" : 1e-5,\n",
    "          \"forward_activation_function\" : 'tanh' ,\n",
    "          \"backward_activation_function\" : 'ReLU' }\n",
    "          \n",
    "        Supported activation\n",
    "        \"\"\"\n",
    "        self.hyperparam = hyperparam  # assume include nrow in dict\n",
    "\n",
    "        # Explicit hyperparameter attributes\n",
    "        self.L = hyperparam[\"L\"]\n",
    "        self.lr = hyperparam[\"lr\"]\n",
    "        self.forward_activation_function = hyperparam[\"forward_activation_function\"]\n",
    "        self.backward_activation_function = hyperparam[\"backward_activation_function\"]\n",
    "        \n",
    "    def compiles(self, initialization = 'random' , optimizer='adam', loss='binary_cross_entropy'):\n",
    "        \"\"\"\n",
    "        Develop soon (optional hyperparameter)\n",
    "        \"\"\"\n",
    "        self.initialization = initialization\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        \n",
    "    def fit(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        Y: pd.Series,\n",
    "        Epochs: int = 1000,\n",
    "        verbose: bool = True,\n",
    "        warmup: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fit the launched Deep L layer with the given data X , Y\n",
    "\n",
    "        Arguments:\n",
    "         X --- Pandas Dataframe of predictors\n",
    "         Y --- Pandas Series of response (0 : negative, 1:positive)\n",
    "         Epoch --- number of epochs (default : 1000)\n",
    "         verbose --- report the epochs every 1000 epoch\n",
    "         warmup --- update parameters and save the parameter\n",
    "        \"\"\"\n",
    "\n",
    "        ## First, we initiate the attributes\n",
    "\n",
    "        # We turn Dataframe into Numpy format\n",
    "        X = X.to_numpy().T\n",
    "        Y = Y.to_numpy().T\n",
    "        nrow = np.shape(X)[0]\n",
    "\n",
    "        # Assign class attribute\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.m = Y.shape[1]\n",
    "        self.Epochs = Epochs\n",
    "\n",
    "        self.param = initiate_param(hyperparam = self.hyperparam,\n",
    "                                        initialization = self.initialization)\n",
    "        for epoch in range(self.Epochs):\n",
    "            A, cache = L_model_forward(self.X, self.param, \n",
    "                                       activation_function=self.forward_activation_function)\n",
    "            #print(A)\n",
    "            if verbose and epoch % 1000 == 0:\n",
    "                cost_reporter(A, self.Y, self.m, epoch, self.Epochs)\n",
    "\n",
    "            grads = L_model_backward(A, self.Y, cache, \n",
    "                                     self.backward_activation_function)\n",
    "            \n",
    "            self.param = update_param(self.param, grads, lr=self.lr)\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Predict probability of the observation given input X\n",
    "\n",
    "        Arguments:\n",
    "         X --- Pandas Dataframe or Series of predictors\n",
    "        \"\"\"\n",
    "        X = X.to_numpy().T\n",
    "\n",
    "        A_prob, _ = L_model_forward(X, self.param, activation_function=self.forward_activation_function)\n",
    "\n",
    "        return A_prob\n",
    "\n",
    "    def predict(self, X, threshold: float = 0.5):\n",
    "        \"\"\"\n",
    "        Predict the observation given input X\n",
    "\n",
    "        Arguments:\n",
    "         X --- Pandas Dataframe or Series of predictors\n",
    "        \"\"\"\n",
    "        A_prob = self.predict_proba(X)\n",
    "        A_pred = thresholder(A_prob, threshold)\n",
    "        return A_pred\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Deep_L_Layer({self.hyperparam})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"A Deep {self.L} Neural network with learning rate = {self.lr} (Forward activation :{self.forward_activation_function},Backward activation :{self.backward_activation_function})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "806lCdjxVXKL"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNaqbnp7VaTC"
   },
   "source": [
    "Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/two_circle.csv')\n",
    "df = df.astype({\"Y\":'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1     float64\n",
       "X2     float64\n",
       "Y     category\n",
       "dtype: object"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "hyperparam = {\"L\" : 5,\n",
    "              \"n_unit\" : [3,8,6,4,2,1],\n",
    "              \"lr\" : 1e-5,\n",
    "              \"forward_activation_function\" : 'sigmoid',\n",
    "              \"backward_activation_function\" : 'sigmoid',\n",
    "              \"keep_prob_sequence\" : [1,0.5,0.6,0.7,1,1]}   #Dropout => No dropout = None OR [1,1,1,1,1,1]\n",
    "\"\"\"\n",
    "X = df[['X1','X2']]\n",
    "Y = df[['Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam = {\"L\" : 5,\n",
    "              \"n_unit\" : [2,8,6,4,2,1],\n",
    "              \"lr\" : 1e-4,\n",
    "              \"forward_activation_function\" : 'ReLU',\n",
    "              \"backward_activation_function\" : 'ReLU',\n",
    "              \"keep_prob_sequence\" : [1,0.5,0.6,0.7,1,1] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Binary_Deep_L_Layer(hyperparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100000 : ===Cost=== : 0.6998931011884904\n",
      "Epoch 1000/100000 : ===Cost=== : 0.696721470129203\n",
      "Epoch 2000/100000 : ===Cost=== : 0.694537218911716\n",
      "Epoch 3000/100000 : ===Cost=== : 0.6935156283998061\n",
      "Epoch 4000/100000 : ===Cost=== : 0.6934197172855407\n",
      "Epoch 5000/100000 : ===Cost=== : 0.6933453395075682\n",
      "Epoch 6000/100000 : ===Cost=== : 0.6932872416904683\n",
      "Epoch 7000/100000 : ===Cost=== : 0.693241483704385\n",
      "Epoch 8000/100000 : ===Cost=== : 0.6932048837441984\n",
      "Epoch 9000/100000 : ===Cost=== : 0.6931752056159535\n",
      "Epoch 10000/100000 : ===Cost=== : 0.6931510587478827\n",
      "Epoch 11000/100000 : ===Cost=== : 0.6931309508103344\n",
      "Epoch 12000/100000 : ===Cost=== : 0.6931138678636728\n",
      "Epoch 13000/100000 : ===Cost=== : 0.69309913218949\n",
      "Epoch 14000/100000 : ===Cost=== : 0.6930861661866989\n",
      "Epoch 15000/100000 : ===Cost=== : 0.6930744942462375\n",
      "Epoch 16000/100000 : ===Cost=== : 0.6930682487891338\n",
      "Epoch 17000/100000 : ===Cost=== : 0.6930625140774691\n",
      "Epoch 18000/100000 : ===Cost=== : 0.6930560574462898\n",
      "Epoch 19000/100000 : ===Cost=== : 0.6930493620813232\n",
      "Epoch 20000/100000 : ===Cost=== : 0.6930423571497529\n",
      "Epoch 21000/100000 : ===Cost=== : 0.6930352303321043\n",
      "Epoch 22000/100000 : ===Cost=== : 0.6930285714076188\n",
      "Epoch 23000/100000 : ===Cost=== : 0.693022264221802\n",
      "Epoch 24000/100000 : ===Cost=== : 0.6930161585910356\n",
      "Epoch 25000/100000 : ===Cost=== : 0.6930102836465972\n",
      "Epoch 26000/100000 : ===Cost=== : 0.6930042206671176\n",
      "Epoch 27000/100000 : ===Cost=== : 0.6929981831340077\n",
      "Epoch 28000/100000 : ===Cost=== : 0.692992054469717\n",
      "Epoch 29000/100000 : ===Cost=== : 0.6929851892153395\n",
      "Epoch 30000/100000 : ===Cost=== : 0.6929754000833838\n",
      "Epoch 31000/100000 : ===Cost=== : 0.6929663104551723\n",
      "Epoch 32000/100000 : ===Cost=== : 0.6929561671765119\n",
      "Epoch 33000/100000 : ===Cost=== : 0.6929433236490449\n",
      "Epoch 34000/100000 : ===Cost=== : 0.6929299091631009\n",
      "Epoch 35000/100000 : ===Cost=== : 0.6929144959814698\n",
      "Epoch 36000/100000 : ===Cost=== : 0.6928971250251315\n",
      "Epoch 37000/100000 : ===Cost=== : 0.6928802047186939\n",
      "Epoch 38000/100000 : ===Cost=== : 0.6928624181668934\n",
      "Epoch 39000/100000 : ===Cost=== : 0.6928451393713514\n",
      "Epoch 40000/100000 : ===Cost=== : 0.6928250108738014\n",
      "Epoch 41000/100000 : ===Cost=== : 0.6928052298474593\n",
      "Epoch 42000/100000 : ===Cost=== : 0.6927821121879576\n",
      "Epoch 43000/100000 : ===Cost=== : 0.6927518192659005\n",
      "Epoch 44000/100000 : ===Cost=== : 0.692721571770557\n",
      "Epoch 45000/100000 : ===Cost=== : 0.6926953989305502\n",
      "Epoch 46000/100000 : ===Cost=== : 0.6926642898482966\n",
      "Epoch 47000/100000 : ===Cost=== : 0.6926343833144711\n",
      "Epoch 48000/100000 : ===Cost=== : 0.6926047836028342\n",
      "Epoch 49000/100000 : ===Cost=== : 0.692574850315959\n",
      "Epoch 50000/100000 : ===Cost=== : 0.6925399792866023\n",
      "Epoch 51000/100000 : ===Cost=== : 0.6925035510686022\n",
      "Epoch 52000/100000 : ===Cost=== : 0.6924641210905853\n",
      "Epoch 53000/100000 : ===Cost=== : 0.6924184918321736\n",
      "Epoch 54000/100000 : ===Cost=== : 0.6923758913304592\n",
      "Epoch 55000/100000 : ===Cost=== : 0.6923383199207873\n",
      "Epoch 56000/100000 : ===Cost=== : 0.6922990468687513\n",
      "Epoch 57000/100000 : ===Cost=== : 0.6922602683855245\n",
      "Epoch 58000/100000 : ===Cost=== : 0.6922224039751841\n",
      "Epoch 59000/100000 : ===Cost=== : 0.6921834818614963\n",
      "Epoch 60000/100000 : ===Cost=== : 0.6921449171204437\n",
      "Epoch 61000/100000 : ===Cost=== : 0.6921069035508027\n",
      "Epoch 62000/100000 : ===Cost=== : 0.6920689645873127\n",
      "Epoch 63000/100000 : ===Cost=== : 0.6920298507740771\n",
      "Epoch 64000/100000 : ===Cost=== : 0.6919897322075483\n",
      "Epoch 65000/100000 : ===Cost=== : 0.6919510108275844\n",
      "Epoch 66000/100000 : ===Cost=== : 0.6919128044443607\n",
      "Epoch 67000/100000 : ===Cost=== : 0.6918712186780271\n",
      "Epoch 68000/100000 : ===Cost=== : 0.6918239331669543\n",
      "Epoch 69000/100000 : ===Cost=== : 0.6917785893343973\n",
      "Epoch 70000/100000 : ===Cost=== : 0.6917283695163314\n",
      "Epoch 71000/100000 : ===Cost=== : 0.6916790406820572\n",
      "Epoch 72000/100000 : ===Cost=== : 0.6916307114916762\n",
      "Epoch 73000/100000 : ===Cost=== : 0.6915822797321126\n",
      "Epoch 74000/100000 : ===Cost=== : 0.6915265464083702\n",
      "Epoch 75000/100000 : ===Cost=== : 0.6914721007007645\n",
      "Epoch 76000/100000 : ===Cost=== : 0.6914183115997967\n",
      "Epoch 77000/100000 : ===Cost=== : 0.6913483288156561\n",
      "Epoch 78000/100000 : ===Cost=== : 0.6912834979840681\n",
      "Epoch 79000/100000 : ===Cost=== : 0.691217017653966\n",
      "Epoch 80000/100000 : ===Cost=== : 0.6911490160737679\n",
      "Epoch 81000/100000 : ===Cost=== : 0.6910750866299709\n",
      "Epoch 82000/100000 : ===Cost=== : 0.6909736528790327\n",
      "Epoch 83000/100000 : ===Cost=== : 0.6907188222818068\n",
      "Epoch 84000/100000 : ===Cost=== : 0.6903666161043486\n",
      "Epoch 85000/100000 : ===Cost=== : 0.6900378648293224\n",
      "Epoch 86000/100000 : ===Cost=== : 0.6898324317295768\n",
      "Epoch 87000/100000 : ===Cost=== : 0.689624325832644\n",
      "Epoch 88000/100000 : ===Cost=== : 0.6893541656143278\n",
      "Epoch 89000/100000 : ===Cost=== : 0.6891382526115989\n",
      "Epoch 90000/100000 : ===Cost=== : 0.6889449423932995\n",
      "Epoch 91000/100000 : ===Cost=== : 0.6887822392522015\n",
      "Epoch 92000/100000 : ===Cost=== : 0.6886307490566383\n",
      "Epoch 93000/100000 : ===Cost=== : 0.6884796143886251\n",
      "Epoch 94000/100000 : ===Cost=== : 0.6883308519677156\n",
      "Epoch 95000/100000 : ===Cost=== : 0.6881794554687375\n",
      "Epoch 96000/100000 : ===Cost=== : 0.6880165015288655\n",
      "Epoch 97000/100000 : ===Cost=== : 0.6878220703208788\n",
      "Epoch 98000/100000 : ===Cost=== : 0.687646645706989\n",
      "Epoch 99000/100000 : ===Cost=== : 0.6874735031237071\n"
     ]
    }
   ],
   "source": [
    "model.compiles(initialization ='Xavier')\n",
    "model.fit(X,Y,Epochs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100000 : ===Cost=== : 0.693491968812983\n",
      "Epoch 1000/100000 : ===Cost=== : 0.6934246900128007\n",
      "Epoch 2000/100000 : ===Cost=== : 0.6933705430206774\n",
      "Epoch 3000/100000 : ===Cost=== : 0.6933269631638648\n",
      "Epoch 4000/100000 : ===Cost=== : 0.6932918872709236\n",
      "Epoch 5000/100000 : ===Cost=== : 0.6932636553707707\n",
      "Epoch 6000/100000 : ===Cost=== : 0.6932409317487136\n",
      "Epoch 7000/100000 : ===Cost=== : 0.6932226415151006\n",
      "Epoch 8000/100000 : ===Cost=== : 0.6932079196175813\n",
      "Epoch 9000/100000 : ===Cost=== : 0.6931960698427185\n",
      "Epoch 10000/100000 : ===Cost=== : 0.6931865318415749\n",
      "Epoch 11000/100000 : ===Cost=== : 0.6931788546036555\n",
      "Epoch 12000/100000 : ===Cost=== : 0.6931726751149239\n",
      "Epoch 13000/100000 : ===Cost=== : 0.6931677011847214\n",
      "Epoch 14000/100000 : ===Cost=== : 0.6931636976259891\n",
      "Epoch 15000/100000 : ===Cost=== : 0.6931604751332388\n",
      "Epoch 16000/100000 : ===Cost=== : 0.6931578813311808\n",
      "Epoch 17000/100000 : ===Cost=== : 0.6931557935700874\n",
      "Epoch 18000/100000 : ===Cost=== : 0.6931541131268755\n",
      "Epoch 19000/100000 : ===Cost=== : 0.6931527605375387\n",
      "Epoch 20000/100000 : ===Cost=== : 0.6931516718401559\n",
      "Epoch 21000/100000 : ===Cost=== : 0.6931507955508044\n",
      "Epoch 22000/100000 : ===Cost=== : 0.6931500902293928\n",
      "Epoch 23000/100000 : ===Cost=== : 0.6931495225203304\n",
      "Epoch 24000/100000 : ===Cost=== : 0.6931490655754022\n",
      "Epoch 25000/100000 : ===Cost=== : 0.693148697784298\n",
      "Epoch 26000/100000 : ===Cost=== : 0.6931484017527776\n",
      "Epoch 27000/100000 : ===Cost=== : 0.6931481634801718\n",
      "Epoch 28000/100000 : ===Cost=== : 0.6931479716973328\n",
      "Epoch 29000/100000 : ===Cost=== : 0.6931478173337369\n",
      "Epoch 30000/100000 : ===Cost=== : 0.6931476930885458\n",
      "Epoch 31000/100000 : ===Cost=== : 0.693147593085349\n",
      "Epoch 32000/100000 : ===Cost=== : 0.6931475125942583\n",
      "Epoch 33000/100000 : ===Cost=== : 0.6931474478082229\n",
      "Epoch 34000/100000 : ===Cost=== : 0.6931473956629801\n",
      "Epoch 35000/100000 : ===Cost=== : 0.6931473536921355\n",
      "Epoch 36000/100000 : ===Cost=== : 0.6931473199105154\n",
      "Epoch 37000/100000 : ===Cost=== : 0.6931472927202771\n",
      "Epoch 38000/100000 : ===Cost=== : 0.6931472708353374\n",
      "Epoch 39000/100000 : ===Cost=== : 0.6931472532205462\n",
      "Epoch 40000/100000 : ===Cost=== : 0.6931472390427251\n",
      "Epoch 41000/100000 : ===Cost=== : 0.693147227631262\n",
      "Epoch 42000/100000 : ===Cost=== : 0.6931472184463916\n",
      "Epoch 43000/100000 : ===Cost=== : 0.6931472110536656\n",
      "Epoch 44000/100000 : ===Cost=== : 0.6931472051034036\n",
      "Epoch 45000/100000 : ===Cost=== : 0.6931472003141541\n",
      "Epoch 46000/100000 : ===Cost=== : 0.6931471964593816\n",
      "Epoch 47000/100000 : ===Cost=== : 0.6931471933567519\n",
      "Epoch 48000/100000 : ===Cost=== : 0.6931471908595073\n",
      "Epoch 49000/100000 : ===Cost=== : 0.6931471888495253\n",
      "Epoch 50000/100000 : ===Cost=== : 0.6931471872317316\n",
      "Epoch 51000/100000 : ===Cost=== : 0.6931471859296022\n",
      "Epoch 52000/100000 : ===Cost=== : 0.6931471848815449\n",
      "Epoch 53000/100000 : ===Cost=== : 0.693147184037985\n",
      "Epoch 54000/100000 : ===Cost=== : 0.693147183359021\n",
      "Epoch 55000/100000 : ===Cost=== : 0.6931471828125368\n",
      "Epoch 56000/100000 : ===Cost=== : 0.6931471823726831\n",
      "Epoch 57000/100000 : ===Cost=== : 0.6931471820186539\n",
      "Epoch 58000/100000 : ===Cost=== : 0.6931471817337032\n",
      "Epoch 59000/100000 : ===Cost=== : 0.6931471815043523\n",
      "Epoch 60000/100000 : ===Cost=== : 0.6931471813197525\n",
      "Epoch 61000/100000 : ===Cost=== : 0.693147181171172\n",
      "Epoch 62000/100000 : ===Cost=== : 0.6931471810515827\n",
      "Epoch 63000/100000 : ===Cost=== : 0.6931471809553278\n",
      "Epoch 64000/100000 : ===Cost=== : 0.6931471808778542\n",
      "Epoch 65000/100000 : ===Cost=== : 0.6931471808154973\n",
      "Epoch 66000/100000 : ===Cost=== : 0.6931471807653076\n",
      "Epoch 67000/100000 : ===Cost=== : 0.6931471807249109\n",
      "Epoch 68000/100000 : ===Cost=== : 0.6931471806923964\n",
      "Epoch 69000/100000 : ===Cost=== : 0.6931471806662263\n",
      "Epoch 70000/100000 : ===Cost=== : 0.6931471806451625\n",
      "Epoch 71000/100000 : ===Cost=== : 0.6931471806282086\n",
      "Epoch 72000/100000 : ===Cost=== : 0.6931471806145628\n",
      "Epoch 73000/100000 : ===Cost=== : 0.6931471806035796\n",
      "Epoch 74000/100000 : ===Cost=== : 0.6931471805947396\n",
      "Epoch 75000/100000 : ===Cost=== : 0.6931471805876243\n",
      "Epoch 76000/100000 : ===Cost=== : 0.6931471805818974\n",
      "Epoch 77000/100000 : ===Cost=== : 0.6931471805772877\n",
      "Epoch 78000/100000 : ===Cost=== : 0.6931471805735778\n",
      "Epoch 79000/100000 : ===Cost=== : 0.6931471805705915\n",
      "Epoch 80000/100000 : ===Cost=== : 0.6931471805681881\n",
      "Epoch 81000/100000 : ===Cost=== : 0.6931471805662537\n",
      "Epoch 82000/100000 : ===Cost=== : 0.6931471805646966\n",
      "Epoch 83000/100000 : ===Cost=== : 0.6931471805634433\n",
      "Epoch 84000/100000 : ===Cost=== : 0.6931471805624347\n",
      "Epoch 85000/100000 : ===Cost=== : 0.6931471805616227\n",
      "Epoch 86000/100000 : ===Cost=== : 0.6931471805609694\n",
      "Epoch 87000/100000 : ===Cost=== : 0.6931471805604433\n",
      "Epoch 88000/100000 : ===Cost=== : 0.69314718056002\n",
      "Epoch 89000/100000 : ===Cost=== : 0.6931471805596793\n",
      "Epoch 90000/100000 : ===Cost=== : 0.6931471805594049\n",
      "Epoch 91000/100000 : ===Cost=== : 0.6931471805591842\n",
      "Epoch 92000/100000 : ===Cost=== : 0.6931471805590066\n",
      "Epoch 93000/100000 : ===Cost=== : 0.6931471805588636\n",
      "Epoch 94000/100000 : ===Cost=== : 0.6931471805587486\n",
      "Epoch 95000/100000 : ===Cost=== : 0.6931471805586558\n",
      "Epoch 96000/100000 : ===Cost=== : 0.6931471805585814\n",
      "Epoch 97000/100000 : ===Cost=== : 0.6931471805585212\n",
      "Epoch 98000/100000 : ===Cost=== : 0.693147180558473\n",
      "Epoch 99000/100000 : ===Cost=== : 0.6931471805584342\n"
     ]
    }
   ],
   "source": [
    "model.compiles(initialization ='random')\n",
    "model.fit(X,Y,Epochs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100000 : ===Cost=== : 0.8132616875182228\n",
      "Epoch 1000/100000 : ===Cost=== : 0.7545755424584069\n",
      "Epoch 2000/100000 : ===Cost=== : 0.7236269706687928\n",
      "Epoch 3000/100000 : ===Cost=== : 0.7074477732445416\n",
      "Epoch 4000/100000 : ===Cost=== : 0.6994907372934155\n",
      "Epoch 5000/100000 : ===Cost=== : 0.6958322515038005\n",
      "Epoch 6000/100000 : ===Cost=== : 0.6942442480982707\n",
      "Epoch 7000/100000 : ===Cost=== : 0.6935844073468633\n",
      "Epoch 8000/100000 : ===Cost=== : 0.6933185439334704\n",
      "Epoch 9000/100000 : ===Cost=== : 0.6932136155250931\n",
      "Epoch 10000/100000 : ===Cost=== : 0.69317275776304\n",
      "Epoch 11000/100000 : ===Cost=== : 0.6931569845762032\n",
      "Epoch 12000/100000 : ===Cost=== : 0.6931509282493841\n",
      "Epoch 13000/100000 : ===Cost=== : 0.693148610711339\n",
      "Epoch 14000/100000 : ===Cost=== : 0.6931477257411078\n",
      "Epoch 15000/100000 : ===Cost=== : 0.6931473882497796\n",
      "Epoch 16000/100000 : ===Cost=== : 0.6931472596485344\n",
      "Epoch 17000/100000 : ===Cost=== : 0.6931472106694649\n",
      "Epoch 18000/100000 : ===Cost=== : 0.6931471920210582\n",
      "Epoch 19000/100000 : ===Cost=== : 0.6931471849221733\n",
      "Epoch 20000/100000 : ===Cost=== : 0.6931471822201607\n",
      "Epoch 21000/100000 : ===Cost=== : 0.6931471811917818\n",
      "Epoch 22000/100000 : ===Cost=== : 0.6931471808004013\n",
      "Epoch 23000/100000 : ===Cost=== : 0.6931471806514534\n",
      "Epoch 24000/100000 : ===Cost=== : 0.6931471805947694\n",
      "Epoch 25000/100000 : ===Cost=== : 0.693147180573198\n",
      "Epoch 26000/100000 : ===Cost=== : 0.6931471805649888\n",
      "Epoch 27000/100000 : ===Cost=== : 0.6931471805618645\n",
      "Epoch 28000/100000 : ===Cost=== : 0.6931471805606756\n",
      "Epoch 29000/100000 : ===Cost=== : 0.6931471805602232\n",
      "Epoch 30000/100000 : ===Cost=== : 0.693147180560051\n",
      "Epoch 31000/100000 : ===Cost=== : 0.6931471805599857\n",
      "Epoch 32000/100000 : ===Cost=== : 0.6931471805599607\n",
      "Epoch 33000/100000 : ===Cost=== : 0.6931471805599512\n",
      "Epoch 34000/100000 : ===Cost=== : 0.6931471805599474\n",
      "Epoch 35000/100000 : ===Cost=== : 0.6931471805599461\n",
      "Epoch 36000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 37000/100000 : ===Cost=== : 0.6931471805599453\n",
      "Epoch 38000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 39000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 40000/100000 : ===Cost=== : 0.6931471805599451\n",
      "Epoch 41000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 42000/100000 : ===Cost=== : 0.6931471805599451\n",
      "Epoch 43000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 44000/100000 : ===Cost=== : 0.6931471805599455\n",
      "Epoch 45000/100000 : ===Cost=== : 0.6931471805599453\n",
      "Epoch 46000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 47000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 48000/100000 : ===Cost=== : 0.6931471805599451\n",
      "Epoch 49000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 50000/100000 : ===Cost=== : 0.6931471805599455\n",
      "Epoch 51000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 52000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 53000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 54000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 55000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 56000/100000 : ===Cost=== : 0.6931471805599453\n",
      "Epoch 57000/100000 : ===Cost=== : 0.6931471805599453\n",
      "Epoch 58000/100000 : ===Cost=== : 0.6931471805599451\n",
      "Epoch 59000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 60000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 61000/100000 : ===Cost=== : 0.6931471805599453\n",
      "Epoch 62000/100000 : ===Cost=== : 0.6931471805599453\n",
      "Epoch 63000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 64000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 65000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 66000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 67000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 68000/100000 : ===Cost=== : 0.6931471805599451\n",
      "Epoch 69000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 70000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 71000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 72000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 73000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 74000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 75000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 76000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 77000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 78000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 79000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 80000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 81000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 82000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 83000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 84000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 85000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 86000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 87000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 88000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 89000/100000 : ===Cost=== : 0.6931471805599452\n",
      "Epoch 90000/100000 : ===Cost=== : 0.6931471805599453\n",
      "Epoch 91000/100000 : ===Cost=== : 0.6931471805599453\n",
      "Epoch 92000/100000 : ===Cost=== : 0.6931471805599453\n",
      "Epoch 93000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 94000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 95000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 96000/100000 : ===Cost=== : 0.6931471805599454\n",
      "Epoch 97000/100000 : ===Cost=== : 0.6931471805599455\n",
      "Epoch 98000/100000 : ===Cost=== : 0.6931471805599455\n",
      "Epoch 99000/100000 : ===Cost=== : 0.6931471805599455\n"
     ]
    }
   ],
   "source": [
    "model.compiles(initialization ='zero')\n",
    "model.fit(X,Y,Epochs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/94000 : ===Cost=== : 0.7058925914610181\n",
      "Epoch 1000/94000 : ===Cost=== : 0.6934677382634628\n",
      "Epoch 2000/94000 : ===Cost=== : 0.6919602036058143\n",
      "Epoch 3000/94000 : ===Cost=== : 0.690885146901582\n",
      "Epoch 4000/94000 : ===Cost=== : 0.6899133602023477\n",
      "Epoch 5000/94000 : ===Cost=== : 0.6894031267321077\n",
      "Epoch 6000/94000 : ===Cost=== : 0.6889286458618129\n",
      "Epoch 7000/94000 : ===Cost=== : 0.6884574847896835\n",
      "Epoch 8000/94000 : ===Cost=== : 0.687999200748849\n",
      "Epoch 9000/94000 : ===Cost=== : 0.6875637585323444\n",
      "Epoch 10000/94000 : ===Cost=== : 0.6870989872706124\n",
      "Epoch 11000/94000 : ===Cost=== : 0.6865843635800317\n",
      "Epoch 12000/94000 : ===Cost=== : 0.6860570401172204\n",
      "Epoch 13000/94000 : ===Cost=== : 0.6854522381762872\n",
      "Epoch 14000/94000 : ===Cost=== : 0.6848567587466909\n",
      "Epoch 15000/94000 : ===Cost=== : 0.6842681424772907\n",
      "Epoch 16000/94000 : ===Cost=== : 0.6837119753515224\n",
      "Epoch 17000/94000 : ===Cost=== : 0.6831544647209948\n",
      "Epoch 18000/94000 : ===Cost=== : 0.6825931061377507\n",
      "Epoch 19000/94000 : ===Cost=== : 0.682035349028375\n",
      "Epoch 20000/94000 : ===Cost=== : 0.6814763947179091\n",
      "Epoch 21000/94000 : ===Cost=== : 0.6808682204116612\n",
      "Epoch 22000/94000 : ===Cost=== : 0.6802160234288854\n",
      "Epoch 23000/94000 : ===Cost=== : 0.67958623596022\n",
      "Epoch 24000/94000 : ===Cost=== : 0.6789658229701105\n",
      "Epoch 25000/94000 : ===Cost=== : 0.6783278578752882\n",
      "Epoch 26000/94000 : ===Cost=== : 0.6776752650169248\n",
      "Epoch 27000/94000 : ===Cost=== : 0.677040713449915\n",
      "Epoch 28000/94000 : ===Cost=== : 0.6763928843974872\n",
      "Epoch 29000/94000 : ===Cost=== : 0.6757360949227398\n",
      "Epoch 30000/94000 : ===Cost=== : 0.6750603652191858\n",
      "Epoch 31000/94000 : ===Cost=== : 0.6743823204418039\n",
      "Epoch 32000/94000 : ===Cost=== : 0.6736848395253071\n",
      "Epoch 33000/94000 : ===Cost=== : 0.6728851265036206\n",
      "Epoch 34000/94000 : ===Cost=== : 0.6720400322112665\n",
      "Epoch 35000/94000 : ===Cost=== : 0.6711177705725977\n",
      "Epoch 36000/94000 : ===Cost=== : 0.6701478024806977\n",
      "Epoch 37000/94000 : ===Cost=== : 0.6691232112296849\n",
      "Epoch 38000/94000 : ===Cost=== : 0.6681351614074708\n",
      "Epoch 39000/94000 : ===Cost=== : 0.6669980091797896\n",
      "Epoch 40000/94000 : ===Cost=== : 0.6653226276963824\n",
      "Epoch 41000/94000 : ===Cost=== : 0.6640161585053905\n",
      "Epoch 42000/94000 : ===Cost=== : 0.6627732121291079\n",
      "Epoch 43000/94000 : ===Cost=== : 0.661478215635902\n",
      "Epoch 44000/94000 : ===Cost=== : 0.6601483439084808\n",
      "Epoch 45000/94000 : ===Cost=== : 0.6587465407796035\n",
      "Epoch 46000/94000 : ===Cost=== : 0.6573097423943968\n",
      "Epoch 47000/94000 : ===Cost=== : 0.6558576194564529\n",
      "Epoch 48000/94000 : ===Cost=== : 0.6543798013636075\n",
      "Epoch 49000/94000 : ===Cost=== : 0.6526278485587467\n",
      "Epoch 50000/94000 : ===Cost=== : 0.6509097873682297\n",
      "Epoch 51000/94000 : ===Cost=== : 0.649238739292866\n",
      "Epoch 52000/94000 : ===Cost=== : 0.647355324544459\n",
      "Epoch 53000/94000 : ===Cost=== : 0.645560466533276\n",
      "Epoch 54000/94000 : ===Cost=== : 0.643670358690186\n",
      "Epoch 55000/94000 : ===Cost=== : 0.6417312941821792\n",
      "Epoch 56000/94000 : ===Cost=== : 0.6396915882968015\n",
      "Epoch 57000/94000 : ===Cost=== : 0.6375879739071341\n",
      "Epoch 58000/94000 : ===Cost=== : 0.6354632634579209\n",
      "Epoch 59000/94000 : ===Cost=== : 0.6331508138646333\n",
      "Epoch 60000/94000 : ===Cost=== : 0.6306782545341914\n",
      "Epoch 61000/94000 : ===Cost=== : 0.628126659800708\n",
      "Epoch 62000/94000 : ===Cost=== : 0.6254688673587872\n",
      "Epoch 63000/94000 : ===Cost=== : 0.6224317496240523\n",
      "Epoch 64000/94000 : ===Cost=== : 0.6186590748269561\n",
      "Epoch 65000/94000 : ===Cost=== : 0.6088309454990006\n",
      "Epoch 66000/94000 : ===Cost=== : 0.5957305295269295\n",
      "Epoch 67000/94000 : ===Cost=== : 0.5834135781586709\n",
      "Epoch 68000/94000 : ===Cost=== : 0.5612249094094487\n",
      "Epoch 69000/94000 : ===Cost=== : 0.5439823390536701\n",
      "Epoch 70000/94000 : ===Cost=== : 0.5312523379394914\n",
      "Epoch 71000/94000 : ===Cost=== : 0.5195041200073395\n",
      "Epoch 72000/94000 : ===Cost=== : 0.509990724926223\n",
      "Epoch 73000/94000 : ===Cost=== : 0.5007843529035463\n",
      "Epoch 74000/94000 : ===Cost=== : 0.4915692654136753\n",
      "Epoch 75000/94000 : ===Cost=== : 0.48213669402951054\n",
      "Epoch 76000/94000 : ===Cost=== : 0.4730702399252848\n",
      "Epoch 77000/94000 : ===Cost=== : 0.4644143466958668\n",
      "Epoch 78000/94000 : ===Cost=== : 0.4558575417942724\n",
      "Epoch 79000/94000 : ===Cost=== : 0.44555685278277357\n",
      "Epoch 80000/94000 : ===Cost=== : 0.43606061492429393\n",
      "Epoch 81000/94000 : ===Cost=== : 0.42946381703247033\n",
      "Epoch 82000/94000 : ===Cost=== : 0.42188923445500026\n",
      "Epoch 83000/94000 : ===Cost=== : 0.41769802057603583\n",
      "Epoch 84000/94000 : ===Cost=== : 0.41238116866710284\n",
      "Epoch 85000/94000 : ===Cost=== : 0.4095199478496233\n",
      "Epoch 86000/94000 : ===Cost=== : 0.4065530825130986\n",
      "Epoch 87000/94000 : ===Cost=== : 0.4031323732294387\n",
      "Epoch 88000/94000 : ===Cost=== : 0.4021385183429399\n",
      "Epoch 89000/94000 : ===Cost=== : 0.400740412942217\n",
      "Epoch 90000/94000 : ===Cost=== : 0.3956267954984503\n",
      "Epoch 91000/94000 : ===Cost=== : 0.3899372404618802\n",
      "Epoch 92000/94000 : ===Cost=== : 0.38124934356586104\n",
      "Epoch 93000/94000 : ===Cost=== : 0.3708263854194519\n"
     ]
    }
   ],
   "source": [
    "model.compiles(initialization ='He')\n",
    "model.fit(X,Y,Epochs=94000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X)\n",
    "y = Y.values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = y - Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9425"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy\n",
    "np.count_nonzero(arr==0) / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'c' argument has 400 elements, which is inconsistent with 'x' and 'y' with size 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36m_parse_scatter_color_args\u001b[1;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[0;32m   4349\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Is 'c' acceptable as PathCollection facecolors?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4350\u001b[1;33m                 \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_rgba_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4351\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36mto_rgba_array\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mrgba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mrgba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrgba\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Suppress exception chaining of cache lookup failure.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mrgba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_rgba_no_colorcycle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[1;34m(c, alpha)\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RGBA sequence should have length 3 or 4\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: RGBA sequence should have length 3 or 4",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3292/985875474.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model with He initialization\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplot_decision_boundary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpredict_dec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3292/3205653447.py\u001b[0m in \u001b[0;36mplot_decision_boundary\u001b[1;34m(model, X, y)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSpectral\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[0;32m   3066\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m         edgecolors=None, plotnonfinite=False, data=None, **kwargs):\n\u001b[1;32m-> 3068\u001b[1;33m     __ret = gca().scatter(\n\u001b[0m\u001b[0;32m   3069\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3070\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[0;32m   4514\u001b[0m             \u001b[0morig_edgecolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'edgecolor'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4515\u001b[0m         \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolors\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4516\u001b[1;33m             self._parse_scatter_color_args(\n\u001b[0m\u001b[0;32m   4517\u001b[0m                 \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4518\u001b[0m                 get_next_color_func=self._get_patches_for_fill.get_next_color)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36m_parse_scatter_color_args\u001b[1;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[0;32m   4354\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4355\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalid_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4356\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0minvalid_shape_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxsize\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4357\u001b[0m                     \u001b[1;31m# Both the mapping *and* the RGBA conversion failed: pretty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4358\u001b[0m                     \u001b[1;31m# severe failure => one may appreciate a verbose feedback.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'c' argument has 400 elements, which is inconsistent with 'x' and 'y' with size 2."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Model with He initialization\")\n",
    "axes = plt.gca()\n",
    "plot_decision_boundary(lambda x: predict_dec(model.param, x.T), X.values, Y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(pd.DataFrame(np.array([[0,0.7]])))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Evolution model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
