{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivD1GMJGPCtw"
   },
   "source": [
    "status : Add Weight Init to Deep - L - Layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mPHVQ83CMBNN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WL3pYvQuME-k"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Error & Updating\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def binary_cross_entropy(a, y):\n",
    "    return -((y * np.log(a)) + ((1 - y) * np.log(1 - a)))\n",
    "\n",
    "def update_param(param,dparam,lr):\n",
    "    pass\n",
    "\n",
    "\"\"\"\n",
    "Activation Function\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def tanh(z):\n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def ReLU(z):\n",
    "    return np.where(z >= 0, z, 0)\n",
    "\n",
    "\n",
    "def LeakyReLU(z: float):\n",
    "    return np.where(z >= 0, z, 0.01 * z)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Derivative of Activation Function wrp. Z\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def dReLU(z: float):\n",
    "    return np.where(z >= 0, 1, 0)\n",
    "\n",
    "\n",
    "def dLeakyReLU(z: float):\n",
    "    return np.where(z >= 0, 1, 0.01)\n",
    "\n",
    "\n",
    "def dsigmoid(z: float):\n",
    "    a = sigmoid(z)\n",
    "    return a*(1 - a)\n",
    "\n",
    "\n",
    "def dTanh(z: float):\n",
    "    a = tanh(z)\n",
    "    return 1 - a ** 2\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For Bi-Deep L layer Classification\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def cut_off_threshold(A, thr):\n",
    "    return np.where(A >= thr, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def predict_dec(param, X):\n",
    "    \"\"\"\n",
    "    Used for plotting decision boundary.\n",
    "    \n",
    "    Arguments:\n",
    "    param -- python dictionary containing your param \n",
    "    X -- input data of size (m, K)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Predict using forward propagation and a classification threshold of 0.5\n",
    "    a3, cache = L_model_forward(X, param)\n",
    "    predictions = (a3>0.5)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Sqh2b6lWMFXx"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initiate parameter\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def initiate_param(layer_dims,initialization = 'random',seed:int=42):\n",
    "    \"\"\"Initiate the paramaters W, B for each layer\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "        layer_dims : list\n",
    "            A sequence of number of units for every layer \n",
    "        initialization : str, optional\n",
    "            A technique of weight initialization (default:random)\n",
    "        seed : int, optional \n",
    "            A seed for randomize the initialization\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "        param : numpy.array\n",
    "            Array of parameter of every layer \n",
    "    \n",
    "    \"\"\"\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # 4 Options of weight initializating\n",
    "    if initialization == 'zero':\n",
    "        param = initialization_zero(layer_dims)\n",
    "    elif initialization == 'random':\n",
    "        param = initialization_random(layer_dims)\n",
    "    elif initialization == 'He':\n",
    "        param = initialization_he(layer_dims)\n",
    "    elif initialization == 'Xavier':    \n",
    "        param = initialization_xavier(layer_dims)\n",
    "    else: #default : random\n",
    "        print(f'''There is no weight initialization called \"{initialization}\"\n",
    "              switch to default initialization random\n",
    "              ''')\n",
    "        param = initialization_random(layer_dims)\n",
    "        \n",
    "    return param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_zero(layer_dims:list):\n",
    "    \"\"\"Initialize both weight and bias as zeros\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "    layer_dims : int\n",
    "        A sequence of number of units for every layer \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    param : \n",
    "        Array of parameter of every layer \n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(layer_dims) - 1  #Exclude input layer to calculating L\n",
    "    param = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        param[\"W\" + str(l)] = np.zeros(shape=(layer_dims[l], layer_dims[l-1])) * 0.01 # Uniform(0,1] * 0.01\n",
    "        param[\"b\" + str(l)] = np.zeros(shape=(layer_dims[l], 1))\n",
    "        \n",
    "        assert(param['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(param['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return param\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_random(layer_dims:list,scale:int=0.01):\n",
    "    \n",
    "    \"\"\"Initialize weight randomly with Normal(mean=0,sigma=1)\n",
    "    Initialize bias as uniform distributed ( min=0,max= <1 )\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "    layer_dimss : int\n",
    "        A sequence of number of units for every layer \n",
    "    scale : float, optional\n",
    "        A constant to scale the weight initialization\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    param : \n",
    "        Array of parameter of every layer \n",
    "    \"\"\"\n",
    "    L = len(layer_dims) - 1  #Exclude input layer to calculating L\n",
    "    param = {}\n",
    "    \n",
    "    \"\"\"\n",
    "    scale : variance of the random variable\n",
    "    y = scale * x\n",
    "    var(y) = var(scale*x)\n",
    "    var(y) = scale^2 * x\n",
    "    \"\"\"\n",
    "    for l in range(1,L+1):\n",
    "        param[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * scale # Normal(0,1) * scale \n",
    "        param[\"b\" + str(l)] = np.random.rand(layer_dims[l], 1)\n",
    "        \n",
    "        assert(param['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(param['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return param\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_xavier(layer_dims:list):\n",
    "    \"\"\"\n",
    "    Initialize weight randomly with Normal(mean=0,sigma=(1/fan_avg))\n",
    "    Initialize bias as uniform distributed ( min=0,max= <1 )\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "    layer_dimss : int\n",
    "        A sequence of number of units for every layer \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    param : \n",
    "        Array of parameter of every layer \n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(layer_dims) - 1  #Exclude input layer to calculating L\n",
    "    param = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        fan_in , fan_out = layer_dims[l-1] , layer_dims[l]\n",
    "        fan_avg = 1/2 * (fan_in + fan_out)\n",
    "        \n",
    "        param[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(1/fan_avg) \n",
    "        param[\"b\" + str(l)] =  np.random.rand(layer_dims[l], 1)\n",
    "        \n",
    "        assert(param['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(param['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return param\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_he(layer_dims:list):\n",
    "    \"\"\"\n",
    "    Initialize weight randomly with Normal(mean=0,sigma=(2/fan_in))\n",
    "    Initialize bias as uniform distributed ( min=0,max= <1 )\n",
    "    \n",
    "    Arguments\n",
    "    ----------\n",
    "    layer_dimss : int\n",
    "        A sequence of number of units for every layer \n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    param : \n",
    "        Array of parameter of every layer \n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(layer_dims) - 1  #Exclude input layer to calculating L\n",
    "    param = {}\n",
    "    \n",
    "    for l in range(1,L+1):\n",
    "        fan_in = layer_dims[l-1]\n",
    "        \n",
    "        param[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/fan_in) \n",
    "        param[\"b\" + str(l)] =  np.random.rand(layer_dims[l], 1)\n",
    "        \n",
    "        assert(param['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(param['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return param\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hCAaYiE0MIfs"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Forward Propagation Unit\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def linear_forward(A_prev, W, b):\n",
    "    \"\"\"Linear Forward unit\n",
    "    \n",
    "    Argument\n",
    "    ----------    \n",
    "    1. A_prev --- Activation node of the previous layer A[l-1]\n",
    "    2. W --- Weight of layer l\n",
    "    3. b --- Bias of layer l\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    1. Z --- Output Z of layer l \n",
    "    2. caches --- cache of Linear forward Unit\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (A_prev,W,b)      # A :for dZ, W for dA & to get updating, b for updating , dA for dZ\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation_function):\n",
    "    \"\"\"Linear Forward unit\n",
    "    \n",
    "    Argument\n",
    "    ----------    \n",
    "    1. A_prev --- Activation node of the previous layer A[l-1]\n",
    "    2. W --- Weight of layer l\n",
    "    3. b --- Bias of layer l\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    1. Z --- Output Z of layer l \n",
    "    2. caches --- cache of Linear forward Unit and Activation function\n",
    "    \"\"\"\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "\n",
    "    if activation_function == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    elif activation_function == \"tanh\":\n",
    "        A = tanh(Z)\n",
    "    elif activation_function == \"ReLU\":\n",
    "        A = ReLU(Z)\n",
    "    elif activation_function == \"LeakyReLU\":\n",
    "        A = LeakyReLU(Z)\n",
    "    elif activation_function == \"linear\":\n",
    "        A = Z\n",
    "    \n",
    "    activation_cache = Z\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (activation_cache, linear_cache)  # (Z, (A_prev,W,b))\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def L_model_forward(X, param, activation_function=\"ReLU\", last_activation_function=\"sigmoid\"):\n",
    "    \"\"\"Forward propagation model from input to output layer\n",
    "       Apply parameter to the input X to return the Activation Output \n",
    "    \n",
    "    Argument\n",
    "    ----------    \n",
    "    1. X --- Input denoted as A[0]\n",
    "    2. param --- Weight and Bias of every layer\n",
    "    3. activation_function --- the activation function for hidden layer (default:ReLU)\n",
    "    4. last_activation_function --- the activation function for output layer (default:sigmoid)\n",
    "                                    Classication : sigmoid\n",
    "                                    Regression : linear\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    1. AL --- Output A[L] from the propagation (Z[L] with sigmoid activation function)\n",
    "    2. caches --- the cache of every layer l \n",
    "    \"\"\"\n",
    "\n",
    "    A = X\n",
    "    L = (len(param) // 2)  # param stores the weight and bias for L layer, hence len(param) = 2L\n",
    "\n",
    "    caches = []\n",
    "    \n",
    "    # For Hidden Layer [1,2..,L-1]\n",
    "    for l in range(1,L):  # l = 1,2,..,L-1\n",
    "        A_prev = A\n",
    "        W = param[\"W\" + str(l)]\n",
    "        b = param[\"b\" + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, activation_function)\n",
    "        caches.append(cache)  # append cache at layer l\n",
    "    \n",
    "    # For Output layer [L]\n",
    "\n",
    "    A_prev = A\n",
    "    W = param[\"W\" + str(L)]\n",
    "    b = param[\"b\" + str(L)]\n",
    "    AL, cache = linear_activation_forward(A_prev, W, b, last_activation_function)\n",
    "    caches.append(cache)\n",
    "\n",
    "    \n",
    "    assert(AL.shape == (1, X.shape[1]))\n",
    "\n",
    "    return AL, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MZtJnWszMTkj"
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Compute the cost function with respect to tAL\n",
    "    cost function : Binary cross entropy\n",
    "    Arguments:\n",
    "    A --- predicted value from L-Forward model\n",
    "    y --- actual output\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    loss = binary_cross_entropy(AL, Y)\n",
    "    cost = np.divide(loss, m)  # No significant difference in speed when compare to '/' though\n",
    "    cost = np.sum(cost, axis=1)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CrtGzWENMMh1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Backward Propagation Unit\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"Use dZ from the layer l to obtain dW,dB,dA_prev\n",
    "    Arguments\n",
    "    ----------\n",
    "      dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "      cache -- tuple of values (Z,(A_prev, W, b)) coming from the forward propagation in the current layer (We use only linear cache anyway)\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "      dA_prev --- Gradient of the cost with respect to the activation node at the previous layer\n",
    "      dW --- Gradient of the cost with the weight in this layer\n",
    "      db --- Gradient of the cost with the bias in this layer\n",
    "    \"\"\"\n",
    "    _, linear_cache = cache  # We use only linear cache\n",
    "    (A_prev, W, b) = linear_cache  # We do not use b to obtain those 3 gradients\n",
    "\n",
    "    m = dZ.shape[1]  \n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation_function):\n",
    "    \"\"\"Input dA to find dZ, then use dZ to obtain dW,dB,dA_prev\n",
    "    Arguments\n",
    "    ----------\n",
    "      dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "      cache -- tuple of values (Z,(A_prev, W, b)) coming from the forward propagation in the current layer (We use only linear cache anyway)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "      dA_prev --- Gradient of the cost with respect to the activation node at the previous layer\n",
    "      dW --- Gradient of the cost with the weight in this layer\n",
    "      db --- Gradient of the cost with the bias in this layer\n",
    "    \"\"\"\n",
    "    if activation_function == \"ReLU\":\n",
    "        g_ = dReLU\n",
    "    elif activation_function == \"LeakyReLU\":\n",
    "        g_ = dLeakyReLU\n",
    "    elif activation_function == \"tanh\":\n",
    "        g_ = dTanh\n",
    "    elif activation_function == \"sigmoid\":\n",
    "        g_ = dsigmoid\n",
    "    else:\n",
    "        print(f\"The activation function {activation_function} not found, ReLU as default\")\n",
    "        g_ = dReLU\n",
    "\n",
    "    activation_cache, _ = cache  # We use only activation cache\n",
    "    Z = activation_cache\n",
    "\n",
    "    dZ = dA * g_(Z)\n",
    "    dA_prev, dW, db = linear_backward(dZ, cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def L_model_backward(AL, Y, cache, activation_function=\"ReLU\", last_activation_function=\"sigmoid\"):\n",
    "    \"\"\"\n",
    "    Backward propagation model from output AL to the parameter gradient of all layers\n",
    "    Apply parameter to the input X to return the Activation Output \n",
    "    \n",
    "    Arguments:\n",
    "    A --- A at the layer L\n",
    "    y --- an actual output\n",
    "    cache --- cache from the forward propagation\n",
    "    activation_function --- activation function for the hidden layer\n",
    "    Return:\n",
    "     grads  -- A dictionary with the gradients\n",
    "               grads[\"dA\" + str(l)] = ...\n",
    "               grads[\"dW\" + str(l)] = ...\n",
    "               grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    L = len(cache)  # cache for each layer\n",
    "    grads = {}\n",
    "    \n",
    "    # For Output layer\n",
    "    dAL = np.divide(1 - Y, 1 - AL) - np.divide(Y, AL)  # dA_[L] : Input for the first linear activation backward\n",
    "                                                    # Loss : Binary Cross Entropy\n",
    "    \n",
    "    current_cache = cache[-1] \n",
    "    dA_prev, dW, db = linear_activation_backward(dAL,current_cache,last_activation_function)\n",
    "    grads[\"dW\" + str(L)] = dW\n",
    "    grads[\"db\" + str(L)] = db\n",
    "    \n",
    "    dA = dA_prev\n",
    "    \n",
    "    \n",
    "    # For Hidden layer [L-1, L-2...,1]\n",
    "    for l in reversed(range(1,L)): \n",
    "\n",
    "        current_cache = cache[l-1] \n",
    "        (activation_cache, linear_cache) = current_cache\n",
    "        \n",
    "        Z = activation_cache\n",
    "        a_prev, W, b = linear_cache  # Start with Z_[L] , A_[L-1], W_[L], b_[L]\n",
    "        \n",
    "        dA_prev, dW, db = linear_activation_backward(dA, current_cache, activation_function)\n",
    "\n",
    "        grads[\"dW\" + str(l)] = dW\n",
    "        grads[\"db\" + str(l)] = db\n",
    "        \n",
    "        dA = dA_prev\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-6NkNadCMRfa"
   },
   "outputs": [],
   "source": [
    "def update_param(param, grads, lr=1e-4):\n",
    "    \"\"\"Update parameter \n",
    "    Argument:\n",
    "    1. param -- The current parameter (W1,W2,...,WL,b1,b2,...bL)\n",
    "    2. grads -- the dictionary of gradient that was obtained from L_model_backward function\n",
    "    3. lr (default=1e-4) : Learning rate\n",
    "    Returns:\n",
    "    1. updated_param -- The parameter that got updated\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(param) // 2  # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(1,L+1):\n",
    "        param[\"W\" + str(l)] = (param[\"W\" + str(l)] - lr * grads[\"dW\" + str(l)])\n",
    "        param[\"b\" + str(l)] = (param[\"b\" + str(l)] - lr * grads[\"db\" + str(l)])\n",
    "\n",
    "    return param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71010462])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0.99,0.4,0.3]])\n",
    "b = np.array([[1,1,1]])\n",
    "\n",
    "compute_cost(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_vVbNcrGMV1v",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Binary_Deep_L_Layer:\n",
    "    \"\"\"\n",
    "    A Deep neural network with L layers\n",
    "    - Able to fit with the predictors (X) and the response (Y)\n",
    "    - Able to predict_proba and predict with threshold\n",
    "    To see the last fit model parameter, uses self.param where self refer to the fit model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hyperparam: dict):\n",
    "        \"\"\"\n",
    "        Launch the Deep_L_layer with the given hyperparameter\n",
    "        \n",
    "        Arguments:\n",
    "        hyperparam: A dictionary with key:\n",
    "         L --- Number of Layers (Hidden layer(s) + Output layer)\n",
    "         layer_dims --- Number of units of that L layer\n",
    "         lr --- Learning rate\n",
    "         forward_activation_function --- Activation function for all hidden layer(s) in forward model (ReLU,LeakyReLU,tanh,sigmoid)\n",
    "         last_forward_activation_function --- Activation function for all hidden layer(s) in forward model (sigmoid,linear)\n",
    "          {\"L\" : 5,\n",
    "          \"layer_dims\" : [nrow,8,6,4,2,1],\n",
    "          \"lr\" : 1e-5,\n",
    "          \"forward_activation_function\" : 'tanh' ,\n",
    "          \"last_forward_activation_function\" : 'sigmoid' }\n",
    "          \n",
    "        Supported activation\n",
    "        \"\"\"\n",
    "        self.hyperparam = hyperparam  # assume include nrow in dict\n",
    "\n",
    "        # Explicit hyperparameter attributes\n",
    "        self.L = hyperparam[\"L\"]\n",
    "        self.lr = hyperparam[\"lr\"]\n",
    "        self.forward_activation_function = hyperparam[\"forward_activation_function\"]\n",
    "        self.last_forward_activation_function = hyperparam[\"last_forward_activation_function\"]\n",
    "        \n",
    "    def compiles(self, initialization = 'random' , optimizer='adam', loss='binary_cross_entropy'):\n",
    "        \"\"\"\n",
    "        Develop soon (optional hyperparameter)\n",
    "        \"\"\"\n",
    "        self.initialization = initialization\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        \n",
    "    def fit(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        Y: pd.Series,\n",
    "        Epochs: int = 1000,\n",
    "        report_cost: bool = True,\n",
    "        warmup: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fit the launched Deep L layer with the given data X , Y\n",
    "\n",
    "        Arguments:\n",
    "         X --- Pandas Dataframe of predictors\n",
    "         Y --- Pandas Series of response (0 : negative, 1:positive)\n",
    "         Epoch --- number of epochs (default : 1000)\n",
    "         report_cost --- report the cost epochs every 1000 epoch\n",
    "         warmup --- update param and save the parameter\n",
    "        \"\"\"\n",
    "\n",
    "        ## First, we initiate the attributes\n",
    "\n",
    "        # We turn Dataframe into Numpy format\n",
    "        X = X.to_numpy().T\n",
    "        Y = Y.to_numpy().T\n",
    "        nrow = np.shape(X)[0]\n",
    "\n",
    "        # Assign class attribute\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.m = Y.shape[1]\n",
    "        self.Epochs = Epochs\n",
    "\n",
    "        self.param = initiate_param(layer_dims = self.hyperparam['layer_dims'],\n",
    "                                        initialization = self.initialization)\n",
    "        \n",
    "        for epoch in range(self.Epochs +1):\n",
    "            A, cache = L_model_forward(self.X, self.param, \n",
    "                                       activation_function=self.forward_activation_function,\n",
    "                                      last_activation_function=self.last_forward_activation_function)\n",
    "            #print(A)\n",
    "            if (report_cost and epoch % 1000 == 0):\n",
    "                cost = compute_cost(A, self.Y)\n",
    "                print(f\"Epoch {epoch}/{Epochs} : ===Cost=== : {np.squeeze(cost)}\")\n",
    "\n",
    "            grads = L_model_backward(A, self.Y, cache, \n",
    "                                     self.forward_activation_function,\n",
    "                                    self.last_forward_activation_function)\n",
    "            \n",
    "            self.param = update_param(self.param, grads, lr=self.lr)\n",
    "            \n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Predict probability of the observation given input X\n",
    "\n",
    "        Arguments:\n",
    "         X --- Pandas Dataframe or Series of predictors\n",
    "        \"\"\"\n",
    "        X = X.to_numpy().T\n",
    "\n",
    "        A_prob, _ = L_model_forward(X, self.param, \n",
    "                                    activation_function=self.forward_activation_function,\n",
    "                                    last_activation_function=self.last_forward_activation_function\n",
    "                                   )\n",
    "\n",
    "        return A_prob\n",
    "\n",
    "    def predict(self, X, threshold: float = 0.5,predict_proba=False):\n",
    "        \"\"\"\n",
    "        Predict the observation given input X\n",
    "\n",
    "        Arguments:\n",
    "         X --- Pandas Dataframe or Series of predictors\n",
    "        \"\"\"\n",
    "        \n",
    "        A_prob, _ = L_model_forward(X, self.param, \n",
    "                            activation_function=self.forward_activation_function,\n",
    "                            last_activation_function=self.last_forward_activation_function\n",
    "                           )\n",
    "        \n",
    "        \n",
    "        if not predict_proba:\n",
    "            A_pred = cut_off_threshold(A_prob, threshold)\n",
    "        else:\n",
    "            A_pred = A_prob\n",
    "        return A_pred\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Deep_L_Layer({self.hyperparam})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"A Deep {self.L} Neural network with learning rate = {self.lr} (Forward activation :{self.forward_activation_function},Backward activation :{self.backward_activation_function})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "806lCdjxVXKL"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNaqbnp7VaTC"
   },
   "source": [
    "Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/two_circle.csv')\n",
    "df = df.astype({\"Y\":'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1     float64\n",
       "X2     float64\n",
       "Y     category\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hyperparam = {\"L\" : 5,\n",
    "              \"layer_dims\" : [2,8,6,4,2,1],\n",
    "              \"lr\" : 1e-4,\n",
    "              \"forward_activation_function\" : 'ReLU',\n",
    "              \"last_forward_activation_function\" : 'sigmoid',\n",
    "              \"keep_prob_sequence\" : [1,0.5,0.6,0.7,1,1]}   #Dropout => No dropout = None OR [1,1,1,1,1,1]\n",
    "\n",
    "X = df[['X1','X2']]\n",
    "Y = df[['Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Binary_Deep_L_Layer(hyperparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100000 : ===Cost=== : 0.6998931011884904\n",
      "Epoch 1000/100000 : ===Cost=== : 0.6997832340518138\n",
      "Epoch 2000/100000 : ===Cost=== : 0.6996747689929277\n",
      "Epoch 3000/100000 : ===Cost=== : 0.6995676841487244\n",
      "Epoch 4000/100000 : ===Cost=== : 0.6994652629683484\n",
      "Epoch 5000/100000 : ===Cost=== : 0.6993651000758289\n",
      "Epoch 6000/100000 : ===Cost=== : 0.699268345818704\n",
      "Epoch 7000/100000 : ===Cost=== : 0.6991718760887252\n",
      "Epoch 8000/100000 : ===Cost=== : 0.6990760161394509\n",
      "Epoch 9000/100000 : ===Cost=== : 0.698981248585247\n",
      "Epoch 10000/100000 : ===Cost=== : 0.698887799922073\n",
      "Epoch 11000/100000 : ===Cost=== : 0.6987967709303996\n",
      "Epoch 12000/100000 : ===Cost=== : 0.6987064504244305\n",
      "Epoch 13000/100000 : ===Cost=== : 0.6986173921318232\n",
      "Epoch 14000/100000 : ===Cost=== : 0.6985295760192564\n",
      "Epoch 15000/100000 : ===Cost=== : 0.6984428677820294\n",
      "Epoch 16000/100000 : ===Cost=== : 0.6983571634174042\n",
      "Epoch 17000/100000 : ===Cost=== : 0.6982726456583036\n",
      "Epoch 18000/100000 : ===Cost=== : 0.6981892957676867\n",
      "Epoch 19000/100000 : ===Cost=== : 0.6981070953185076\n",
      "Epoch 20000/100000 : ===Cost=== : 0.6980271895447243\n",
      "Epoch 21000/100000 : ===Cost=== : 0.6979493545304141\n",
      "Epoch 22000/100000 : ===Cost=== : 0.6978726265397972\n",
      "Epoch 23000/100000 : ===Cost=== : 0.6977945508777342\n",
      "Epoch 24000/100000 : ===Cost=== : 0.697717739271202\n",
      "Epoch 25000/100000 : ===Cost=== : 0.6976407230672337\n",
      "Epoch 26000/100000 : ===Cost=== : 0.6975646437004295\n",
      "Epoch 27000/100000 : ===Cost=== : 0.6974932200806916\n",
      "Epoch 28000/100000 : ===Cost=== : 0.697423484274924\n",
      "Epoch 29000/100000 : ===Cost=== : 0.6973547225343891\n",
      "Epoch 30000/100000 : ===Cost=== : 0.6972869195340997\n",
      "Epoch 31000/100000 : ===Cost=== : 0.6972249348627932\n",
      "Epoch 32000/100000 : ===Cost=== : 0.6971663484464149\n",
      "Epoch 33000/100000 : ===Cost=== : 0.6971074034860625\n",
      "Epoch 34000/100000 : ===Cost=== : 0.6970493246896154\n",
      "Epoch 35000/100000 : ===Cost=== : 0.6969949454951152\n",
      "Epoch 36000/100000 : ===Cost=== : 0.69694265530265\n",
      "Epoch 37000/100000 : ===Cost=== : 0.6968920349707167\n",
      "Epoch 38000/100000 : ===Cost=== : 0.6968426287008367\n",
      "Epoch 39000/100000 : ===Cost=== : 0.6967961383170453\n",
      "Epoch 40000/100000 : ===Cost=== : 0.6967490840366094\n",
      "Epoch 41000/100000 : ===Cost=== : 0.6966988709567422\n",
      "Epoch 42000/100000 : ===Cost=== : 0.6966383813444849\n",
      "Epoch 43000/100000 : ===Cost=== : 0.696575903327756\n",
      "Epoch 44000/100000 : ===Cost=== : 0.6965143088787991\n",
      "Epoch 45000/100000 : ===Cost=== : 0.6964535846847102\n",
      "Epoch 46000/100000 : ===Cost=== : 0.6963937176385195\n",
      "Epoch 47000/100000 : ===Cost=== : 0.6963344160944448\n",
      "Epoch 48000/100000 : ===Cost=== : 0.6962721597768518\n",
      "Epoch 49000/100000 : ===Cost=== : 0.6962107773892281\n",
      "Epoch 50000/100000 : ===Cost=== : 0.6961479037171523\n",
      "Epoch 51000/100000 : ===Cost=== : 0.696083976620599\n",
      "Epoch 52000/100000 : ===Cost=== : 0.6960209407589445\n",
      "Epoch 53000/100000 : ===Cost=== : 0.6959634862107615\n",
      "Epoch 54000/100000 : ===Cost=== : 0.6959113355367702\n",
      "Epoch 55000/100000 : ===Cost=== : 0.6958607462336817\n",
      "Epoch 56000/100000 : ===Cost=== : 0.6958103348305622\n",
      "Epoch 57000/100000 : ===Cost=== : 0.6957572200239762\n",
      "Epoch 58000/100000 : ===Cost=== : 0.6957046805693303\n",
      "Epoch 59000/100000 : ===Cost=== : 0.6956472216222939\n",
      "Epoch 60000/100000 : ===Cost=== : 0.6955877807198919\n",
      "Epoch 61000/100000 : ===Cost=== : 0.6955291449485682\n",
      "Epoch 62000/100000 : ===Cost=== : 0.6954713244717294\n",
      "Epoch 63000/100000 : ===Cost=== : 0.6954142597925945\n",
      "Epoch 64000/100000 : ===Cost=== : 0.6953579403064247\n",
      "Epoch 65000/100000 : ===Cost=== : 0.6952979398112069\n",
      "Epoch 66000/100000 : ===Cost=== : 0.6952380515697043\n",
      "Epoch 67000/100000 : ===Cost=== : 0.6951804545828381\n",
      "Epoch 68000/100000 : ===Cost=== : 0.6951314541606403\n",
      "Epoch 69000/100000 : ===Cost=== : 0.6950806888626951\n",
      "Epoch 70000/100000 : ===Cost=== : 0.695028859962288\n",
      "Epoch 71000/100000 : ===Cost=== : 0.6949788954948224\n",
      "Epoch 72000/100000 : ===Cost=== : 0.6949314794355388\n",
      "Epoch 73000/100000 : ===Cost=== : 0.6948830653059324\n",
      "Epoch 74000/100000 : ===Cost=== : 0.6948364273359499\n",
      "Epoch 75000/100000 : ===Cost=== : 0.694796946036042\n",
      "Epoch 76000/100000 : ===Cost=== : 0.6947596263496805\n",
      "Epoch 77000/100000 : ===Cost=== : 0.6947220611363911\n",
      "Epoch 78000/100000 : ===Cost=== : 0.6946802590296135\n",
      "Epoch 79000/100000 : ===Cost=== : 0.6946342140900528\n",
      "Epoch 80000/100000 : ===Cost=== : 0.6945778713610022\n",
      "Epoch 81000/100000 : ===Cost=== : 0.6945277609769894\n",
      "Epoch 82000/100000 : ===Cost=== : 0.6944743477550523\n",
      "Epoch 83000/100000 : ===Cost=== : 0.6944209824089842\n",
      "Epoch 84000/100000 : ===Cost=== : 0.6943711264115116\n",
      "Epoch 85000/100000 : ===Cost=== : 0.694321278142642\n",
      "Epoch 86000/100000 : ===Cost=== : 0.6942664652025288\n",
      "Epoch 87000/100000 : ===Cost=== : 0.6942074417504408\n",
      "Epoch 88000/100000 : ===Cost=== : 0.6941498266746311\n",
      "Epoch 89000/100000 : ===Cost=== : 0.6940900797294258\n",
      "Epoch 90000/100000 : ===Cost=== : 0.6940187266852329\n",
      "Epoch 91000/100000 : ===Cost=== : 0.6939363858286591\n",
      "Epoch 92000/100000 : ===Cost=== : 0.6938668277380553\n",
      "Epoch 93000/100000 : ===Cost=== : 0.6938092789841326\n",
      "Epoch 94000/100000 : ===Cost=== : 0.6937444717449409\n",
      "Epoch 95000/100000 : ===Cost=== : 0.6936864679196826\n",
      "Epoch 96000/100000 : ===Cost=== : 0.6936387840071853\n",
      "Epoch 97000/100000 : ===Cost=== : 0.6935975938634709\n",
      "Epoch 98000/100000 : ===Cost=== : 0.6935826299710737\n",
      "Epoch 99000/100000 : ===Cost=== : 0.6935788512061741\n",
      "Epoch 100000/100000 : ===Cost=== : 0.693575646469504\n"
     ]
    }
   ],
   "source": [
    "#model.compiles(initialization ='Xavier')\n",
    "#model.fit(X,Y,Epochs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.22068604, -0.06343837],\n",
       "        [ 0.29043362,  0.68241276],\n",
       "        [-0.10518428, -0.10550973],\n",
       "        [ 0.70749339,  0.34412656],\n",
       "        [-0.20971881,  0.24326425],\n",
       "        [-0.20758671, -0.20846711],\n",
       "        [ 0.10808282, -0.85560019],\n",
       "        [-0.7721136 , -0.2520617 ]]),\n",
       " 'b1': array([[0.6094902 ],\n",
       "        [0.14342104],\n",
       "        [0.29066899],\n",
       "        [0.36845843],\n",
       "        [0.45753592],\n",
       "        [0.78366867],\n",
       "        [0.1990569 ],\n",
       "        [0.51269844]]),\n",
       " 'W2': array([[-0.20417756,  0.04392019, -0.43469739,  0.14446612, -0.22584358,\n",
       "         -0.10911024, -0.22715673,  0.70040359],\n",
       "        [-0.00512355, -0.39978098,  0.31086887, -0.46143517,  0.07889461,\n",
       "         -0.74074613, -0.5020072 ,  0.07432454],\n",
       "        [ 0.27822602,  0.06339466, -0.04383038, -0.11539837, -0.55953669,\n",
       "         -0.27257183, -0.17415114,  0.39948925],\n",
       "        [ 0.1256563 , -0.66954248,  0.12096747, -0.15005175, -0.25905009,\n",
       "          0.22681177,  0.38935248,  0.35094919],\n",
       "        [-0.31550296, -0.11523898,  0.12552922,  0.37125574, -0.18022715,\n",
       "         -0.06907464, -0.41811983, -0.45212361],\n",
       "        [ 0.30704627,  0.51208749, -0.02711225,  0.37885013,  0.13655685,\n",
       "         -0.24363371,  0.13674554,  0.58147826]]),\n",
       " 'b2': array([[0.77324057],\n",
       "        [0.49373203],\n",
       "        [0.5214814 ],\n",
       "        [0.4209759 ],\n",
       "        [0.02761997],\n",
       "        [0.10784699]]),\n",
       " 'W3': array([[ 0.03485695, -0.13372015,  0.03970879, -0.88823437, -0.09845167,\n",
       "          0.15100081],\n",
       "        [ 0.66269241, -0.23178187, -0.36099837, -0.22485822,  0.40947866,\n",
       "          0.15082517],\n",
       "        [-0.23923095,  0.22951966,  0.0423123 ,  0.43106215, -0.31396769,\n",
       "         -0.14968484],\n",
       "        [-0.17491601, -0.65450568,  0.1325694 ,  0.1165886 ,  0.00231158,\n",
       "         -0.10395091]]),\n",
       " 'b3': array([[0.41157531],\n",
       "        [0.22473136],\n",
       "        [0.11621146],\n",
       "        [0.33828542]]),\n",
       " 'W4': array([[-0.09374093,  0.23251547,  1.08870796,  0.10070081],\n",
       "        [ 0.11384315, -0.08194139, -1.10773214, -0.01881429]]),\n",
       " 'b4': array([[ 0.28393458],\n",
       "        [-0.00824503]]),\n",
       " 'W5': array([[0.03211548, 2.00983269]]),\n",
       " 'b5': array([[0.02949085]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100000 : ===Cost=== : 0.693491968812983\n",
      "Epoch 1000/100000 : ===Cost=== : 0.6934901040486823\n",
      "Epoch 2000/100000 : ===Cost=== : 0.693488249367628\n",
      "Epoch 3000/100000 : ===Cost=== : 0.6934864047153113\n",
      "Epoch 4000/100000 : ===Cost=== : 0.6934845700375185\n",
      "Epoch 5000/100000 : ===Cost=== : 0.693482745280328\n",
      "Epoch 6000/100000 : ===Cost=== : 0.6934809303901105\n",
      "Epoch 7000/100000 : ===Cost=== : 0.6934791253135258\n",
      "Epoch 8000/100000 : ===Cost=== : 0.6934773299975225\n",
      "Epoch 9000/100000 : ===Cost=== : 0.6934755443893358\n",
      "Epoch 10000/100000 : ===Cost=== : 0.6934737684364859\n",
      "Epoch 11000/100000 : ===Cost=== : 0.6934720020867767\n",
      "Epoch 12000/100000 : ===Cost=== : 0.6934702452882944\n",
      "Epoch 13000/100000 : ===Cost=== : 0.6934684979894056\n",
      "Epoch 14000/100000 : ===Cost=== : 0.6934667601387559\n",
      "Epoch 15000/100000 : ===Cost=== : 0.6934650316852686\n",
      "Epoch 16000/100000 : ===Cost=== : 0.6934633125781429\n",
      "Epoch 17000/100000 : ===Cost=== : 0.6934616027668528\n",
      "Epoch 18000/100000 : ===Cost=== : 0.693459902201145\n",
      "Epoch 19000/100000 : ===Cost=== : 0.6934582108310381\n",
      "Epoch 20000/100000 : ===Cost=== : 0.693456528606821\n",
      "Epoch 21000/100000 : ===Cost=== : 0.6934548554790507\n",
      "Epoch 22000/100000 : ===Cost=== : 0.6934531913985519\n",
      "Epoch 23000/100000 : ===Cost=== : 0.6934515363164149\n",
      "Epoch 24000/100000 : ===Cost=== : 0.6934498901839942\n",
      "Epoch 25000/100000 : ===Cost=== : 0.6934482529529076\n",
      "Epoch 26000/100000 : ===Cost=== : 0.6934466245750339\n",
      "Epoch 27000/100000 : ===Cost=== : 0.6934450050025125\n",
      "Epoch 28000/100000 : ===Cost=== : 0.6934433941877411\n",
      "Epoch 29000/100000 : ===Cost=== : 0.6934417920833748\n",
      "Epoch 30000/100000 : ===Cost=== : 0.6934401986423243\n",
      "Epoch 31000/100000 : ===Cost=== : 0.6934386138177556\n",
      "Epoch 32000/100000 : ===Cost=== : 0.6934370375630868\n",
      "Epoch 33000/100000 : ===Cost=== : 0.6934354698319887\n",
      "Epoch 34000/100000 : ===Cost=== : 0.6934339105783814\n",
      "Epoch 35000/100000 : ===Cost=== : 0.6934323597564354\n",
      "Epoch 36000/100000 : ===Cost=== : 0.6934308173205677\n",
      "Epoch 37000/100000 : ===Cost=== : 0.6934292832254423\n",
      "Epoch 38000/100000 : ===Cost=== : 0.693427757425968\n",
      "Epoch 39000/100000 : ===Cost=== : 0.6934262398772977\n",
      "Epoch 40000/100000 : ===Cost=== : 0.693424730534826\n",
      "Epoch 41000/100000 : ===Cost=== : 0.6934232293541889\n",
      "Epoch 42000/100000 : ===Cost=== : 0.6934217362912626\n",
      "Epoch 43000/100000 : ===Cost=== : 0.693420251302161\n",
      "Epoch 44000/100000 : ===Cost=== : 0.693418774343236\n",
      "Epoch 45000/100000 : ===Cost=== : 0.6934173053710746\n",
      "Epoch 46000/100000 : ===Cost=== : 0.693415844342499\n",
      "Epoch 47000/100000 : ===Cost=== : 0.6934143912145646\n",
      "Epoch 48000/100000 : ===Cost=== : 0.693412945944559\n",
      "Epoch 49000/100000 : ===Cost=== : 0.6934115084900003\n",
      "Epoch 50000/100000 : ===Cost=== : 0.6934100788086368\n",
      "Epoch 51000/100000 : ===Cost=== : 0.6934086568584444\n",
      "Epoch 52000/100000 : ===Cost=== : 0.6934072425976269\n",
      "Epoch 53000/100000 : ===Cost=== : 0.6934058359846135\n",
      "Epoch 54000/100000 : ===Cost=== : 0.6934044369780581\n",
      "Epoch 55000/100000 : ===Cost=== : 0.6934030455368385\n",
      "Epoch 56000/100000 : ===Cost=== : 0.6934016616200541\n",
      "Epoch 57000/100000 : ===Cost=== : 0.693400285187026\n",
      "Epoch 58000/100000 : ===Cost=== : 0.6933989161972947\n",
      "Epoch 59000/100000 : ===Cost=== : 0.6933975546106197\n",
      "Epoch 60000/100000 : ===Cost=== : 0.6933962003869777\n",
      "Epoch 61000/100000 : ===Cost=== : 0.6933948534865617\n",
      "Epoch 62000/100000 : ===Cost=== : 0.6933935138697804\n",
      "Epoch 63000/100000 : ===Cost=== : 0.6933921814972559\n",
      "Epoch 64000/100000 : ===Cost=== : 0.6933908563298232\n",
      "Epoch 65000/100000 : ===Cost=== : 0.6933895383285293\n",
      "Epoch 66000/100000 : ===Cost=== : 0.6933882274546315\n",
      "Epoch 67000/100000 : ===Cost=== : 0.6933869236695964\n",
      "Epoch 68000/100000 : ===Cost=== : 0.6933856269350991\n",
      "Epoch 69000/100000 : ===Cost=== : 0.6933843372130217\n",
      "Epoch 70000/100000 : ===Cost=== : 0.6933830544654526\n",
      "Epoch 71000/100000 : ===Cost=== : 0.6933817786546845\n",
      "Epoch 72000/100000 : ===Cost=== : 0.6933805097432145\n",
      "Epoch 73000/100000 : ===Cost=== : 0.6933792476937423\n",
      "Epoch 74000/100000 : ===Cost=== : 0.6933779924691688\n",
      "Epoch 75000/100000 : ===Cost=== : 0.6933767440325957\n",
      "Epoch 76000/100000 : ===Cost=== : 0.6933755023473245\n",
      "Epoch 77000/100000 : ===Cost=== : 0.6933742673768541\n",
      "Epoch 78000/100000 : ===Cost=== : 0.6933730390848819\n",
      "Epoch 79000/100000 : ===Cost=== : 0.6933718174353006\n",
      "Epoch 80000/100000 : ===Cost=== : 0.6933706023921982\n",
      "Epoch 81000/100000 : ===Cost=== : 0.6933693939198571\n",
      "Epoch 82000/100000 : ===Cost=== : 0.6933681919827526\n",
      "Epoch 83000/100000 : ===Cost=== : 0.693366996545552\n",
      "Epoch 84000/100000 : ===Cost=== : 0.6933658075731135\n",
      "Epoch 85000/100000 : ===Cost=== : 0.6933646250304853\n",
      "Epoch 86000/100000 : ===Cost=== : 0.6933634488829048\n",
      "Epoch 87000/100000 : ===Cost=== : 0.6933622790957967\n",
      "Epoch 88000/100000 : ===Cost=== : 0.693361115634773\n",
      "Epoch 89000/100000 : ===Cost=== : 0.6933599584656314\n",
      "Epoch 90000/100000 : ===Cost=== : 0.6933588075543546\n",
      "Epoch 91000/100000 : ===Cost=== : 0.6933576628671092\n",
      "Epoch 92000/100000 : ===Cost=== : 0.6933565243702445\n",
      "Epoch 93000/100000 : ===Cost=== : 0.6933553920302917\n",
      "Epoch 94000/100000 : ===Cost=== : 0.6933542658139629\n",
      "Epoch 95000/100000 : ===Cost=== : 0.6933531456881503\n",
      "Epoch 96000/100000 : ===Cost=== : 0.693352031619925\n",
      "Epoch 97000/100000 : ===Cost=== : 0.6933509235765357\n",
      "Epoch 98000/100000 : ===Cost=== : 0.6933498215254086\n",
      "Epoch 99000/100000 : ===Cost=== : 0.6933487254341459\n",
      "Epoch 100000/100000 : ===Cost=== : 0.6933476352705246\n"
     ]
    }
   ],
   "source": [
    "model.compiles(initialization ='random')\n",
    "model.fit(X,Y,Epochs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 1000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 2000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 3000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 4000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 5000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 6000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 7000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 8000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 9000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 10000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 11000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 12000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 13000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 14000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 15000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 16000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 17000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 18000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 19000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 20000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 21000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 22000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 23000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 24000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 25000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 26000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 27000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 28000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 29000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 30000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 31000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 32000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 33000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 34000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 35000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 36000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 37000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 38000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 39000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 40000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 41000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 42000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 43000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 44000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 45000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 46000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 47000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 48000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 49000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 50000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 51000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 52000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 53000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 54000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 55000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 56000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 57000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 58000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 59000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 60000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 61000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 62000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 63000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 64000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 65000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 66000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 67000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 68000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 69000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 70000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 71000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 72000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 73000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 74000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 75000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 76000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 77000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 78000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 79000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 80000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 81000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 82000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 83000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 84000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 85000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 86000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 87000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 88000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 89000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 90000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 91000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 92000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 93000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 94000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 95000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 96000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 97000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 98000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 99000/100000 : ===Cost=== : 0.6931471805599456\n",
      "Epoch 100000/100000 : ===Cost=== : 0.6931471805599456\n"
     ]
    }
   ],
   "source": [
    "model.compiles(initialization ='zero')\n",
    "model.fit(X,Y,Epochs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100000 : ===Cost=== : 0.7058925914610181\n",
      "Epoch 1000/100000 : ===Cost=== : 0.7003626297627251\n",
      "Epoch 2000/100000 : ===Cost=== : 0.6970405135741111\n",
      "Epoch 3000/100000 : ===Cost=== : 0.694919985424812\n",
      "Epoch 4000/100000 : ===Cost=== : 0.6935116693459789\n",
      "Epoch 5000/100000 : ===Cost=== : 0.6928730865977202\n",
      "Epoch 6000/100000 : ===Cost=== : 0.6925516574807107\n",
      "Epoch 7000/100000 : ===Cost=== : 0.6922527341245531\n",
      "Epoch 8000/100000 : ===Cost=== : 0.6919718436778719\n",
      "Epoch 9000/100000 : ===Cost=== : 0.6917054596840395\n",
      "Epoch 10000/100000 : ===Cost=== : 0.6914388047276112\n",
      "Epoch 11000/100000 : ===Cost=== : 0.6911676050632792\n",
      "Epoch 12000/100000 : ===Cost=== : 0.6908984282554695\n",
      "Epoch 13000/100000 : ===Cost=== : 0.6906416311649397\n",
      "Epoch 14000/100000 : ===Cost=== : 0.6903884708786967\n",
      "Epoch 15000/100000 : ===Cost=== : 0.6901443039003872\n",
      "Epoch 16000/100000 : ===Cost=== : 0.6899215746245715\n",
      "Epoch 17000/100000 : ===Cost=== : 0.6897058163002452\n",
      "Epoch 18000/100000 : ===Cost=== : 0.6894924440947122\n",
      "Epoch 19000/100000 : ===Cost=== : 0.6892796630573259\n",
      "Epoch 20000/100000 : ===Cost=== : 0.6890715616884215\n",
      "Epoch 21000/100000 : ===Cost=== : 0.6888692387554918\n",
      "Epoch 22000/100000 : ===Cost=== : 0.6886801006397287\n",
      "Epoch 23000/100000 : ===Cost=== : 0.6884941153934891\n",
      "Epoch 24000/100000 : ===Cost=== : 0.688310878508922\n",
      "Epoch 25000/100000 : ===Cost=== : 0.6881296758831019\n",
      "Epoch 26000/100000 : ===Cost=== : 0.68794673215707\n",
      "Epoch 27000/100000 : ===Cost=== : 0.6877659988044593\n",
      "Epoch 28000/100000 : ===Cost=== : 0.687594214350428\n",
      "Epoch 29000/100000 : ===Cost=== : 0.6874262779962798\n",
      "Epoch 30000/100000 : ===Cost=== : 0.6872621065526199\n",
      "Epoch 31000/100000 : ===Cost=== : 0.6871012411905455\n",
      "Epoch 32000/100000 : ===Cost=== : 0.6869423471496965\n",
      "Epoch 33000/100000 : ===Cost=== : 0.6867858740677274\n",
      "Epoch 34000/100000 : ===Cost=== : 0.6866298045243543\n",
      "Epoch 35000/100000 : ===Cost=== : 0.686467110640113\n",
      "Epoch 36000/100000 : ===Cost=== : 0.6863028413143009\n",
      "Epoch 37000/100000 : ===Cost=== : 0.6861382183478968\n",
      "Epoch 38000/100000 : ===Cost=== : 0.6859742264950299\n",
      "Epoch 39000/100000 : ===Cost=== : 0.6858057187994938\n",
      "Epoch 40000/100000 : ===Cost=== : 0.6856220903085398\n",
      "Epoch 41000/100000 : ===Cost=== : 0.6854468334640592\n",
      "Epoch 42000/100000 : ===Cost=== : 0.685271842576728\n",
      "Epoch 43000/100000 : ===Cost=== : 0.6850927374688638\n",
      "Epoch 44000/100000 : ===Cost=== : 0.6849094962603881\n",
      "Epoch 45000/100000 : ===Cost=== : 0.6847279634475767\n",
      "Epoch 46000/100000 : ===Cost=== : 0.6845463923386464\n",
      "Epoch 47000/100000 : ===Cost=== : 0.6843637980512454\n",
      "Epoch 48000/100000 : ===Cost=== : 0.6841760348034209\n",
      "Epoch 49000/100000 : ===Cost=== : 0.6839767372758462\n",
      "Epoch 50000/100000 : ===Cost=== : 0.683778305704938\n",
      "Epoch 51000/100000 : ===Cost=== : 0.6835741134356875\n",
      "Epoch 52000/100000 : ===Cost=== : 0.6833667063312645\n",
      "Epoch 53000/100000 : ===Cost=== : 0.6831593561516105\n",
      "Epoch 54000/100000 : ===Cost=== : 0.6829559832062717\n",
      "Epoch 55000/100000 : ===Cost=== : 0.6827581619009829\n",
      "Epoch 56000/100000 : ===Cost=== : 0.6825666540774042\n",
      "Epoch 57000/100000 : ===Cost=== : 0.682375200488788\n",
      "Epoch 58000/100000 : ===Cost=== : 0.6821822914527441\n",
      "Epoch 59000/100000 : ===Cost=== : 0.6819906140378481\n",
      "Epoch 60000/100000 : ===Cost=== : 0.6818016404292958\n",
      "Epoch 61000/100000 : ===Cost=== : 0.6816126757387055\n",
      "Epoch 62000/100000 : ===Cost=== : 0.681423794156137\n",
      "Epoch 63000/100000 : ===Cost=== : 0.6812353965713817\n",
      "Epoch 64000/100000 : ===Cost=== : 0.681047031243712\n",
      "Epoch 65000/100000 : ===Cost=== : 0.6808601622283512\n",
      "Epoch 66000/100000 : ===Cost=== : 0.6806770508919915\n",
      "Epoch 67000/100000 : ===Cost=== : 0.6804933710810483\n",
      "Epoch 68000/100000 : ===Cost=== : 0.6803096673936451\n",
      "Epoch 69000/100000 : ===Cost=== : 0.6801249228889628\n",
      "Epoch 70000/100000 : ===Cost=== : 0.6799327455497017\n",
      "Epoch 71000/100000 : ===Cost=== : 0.6797402572377276\n",
      "Epoch 72000/100000 : ===Cost=== : 0.6795472454005042\n",
      "Epoch 73000/100000 : ===Cost=== : 0.679353637656075\n",
      "Epoch 74000/100000 : ===Cost=== : 0.6791603316960271\n",
      "Epoch 75000/100000 : ===Cost=== : 0.6789663845555418\n",
      "Epoch 76000/100000 : ===Cost=== : 0.6787670816334285\n",
      "Epoch 77000/100000 : ===Cost=== : 0.6785671665317016\n",
      "Epoch 78000/100000 : ===Cost=== : 0.6783665460493702\n",
      "Epoch 79000/100000 : ===Cost=== : 0.6781658120871786\n",
      "Epoch 80000/100000 : ===Cost=== : 0.6779653510111868\n",
      "Epoch 81000/100000 : ===Cost=== : 0.6777641917573534\n",
      "Epoch 82000/100000 : ===Cost=== : 0.677562178560877\n",
      "Epoch 83000/100000 : ===Cost=== : 0.6773589844827528\n",
      "Epoch 84000/100000 : ===Cost=== : 0.6771544253805597\n",
      "Epoch 85000/100000 : ===Cost=== : 0.6769490108751208\n",
      "Epoch 86000/100000 : ===Cost=== : 0.6767433456829174\n",
      "Epoch 87000/100000 : ===Cost=== : 0.6765181805627554\n",
      "Epoch 88000/100000 : ===Cost=== : 0.6762851477579278\n",
      "Epoch 89000/100000 : ===Cost=== : 0.6760655518468935\n",
      "Epoch 90000/100000 : ===Cost=== : 0.6758464591424178\n",
      "Epoch 91000/100000 : ===Cost=== : 0.6756266308540773\n",
      "Epoch 92000/100000 : ===Cost=== : 0.6754017700695383\n",
      "Epoch 93000/100000 : ===Cost=== : 0.675174240916762\n",
      "Epoch 94000/100000 : ===Cost=== : 0.674945268889503\n",
      "Epoch 95000/100000 : ===Cost=== : 0.6747168771273536\n",
      "Epoch 96000/100000 : ===Cost=== : 0.6744944574340994\n",
      "Epoch 97000/100000 : ===Cost=== : 0.6742775730573004\n",
      "Epoch 98000/100000 : ===Cost=== : 0.674064269504819\n",
      "Epoch 99000/100000 : ===Cost=== : 0.6738493159484577\n",
      "Epoch 100000/100000 : ===Cost=== : 0.6736334639604682\n"
     ]
    }
   ],
   "source": [
    "model.compiles(initialization ='He')\n",
    "model.fit(X,Y,Epochs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.49447939, -0.13998468],\n",
       "        [ 0.64763831,  1.5226475 ],\n",
       "        [-0.2333232 , -0.23385667],\n",
       "        [ 1.58087409,  0.76847747],\n",
       "        [-0.46911359,  0.54130353],\n",
       "        [-0.46337517, -0.46583659],\n",
       "        [ 0.2411551 , -1.91296599],\n",
       "        [-1.72514433, -0.56184532]]),\n",
       " 'b1': array([[0.6112786 ],\n",
       "        [0.14091129],\n",
       "        [0.29389715],\n",
       "        [0.36721661],\n",
       "        [0.45562394],\n",
       "        [0.78399416],\n",
       "        [0.19879067],\n",
       "        [0.51386524]]),\n",
       " 'W2': array([[-0.27170122,  0.06075421, -0.57632172,  0.19266985, -0.29952274,\n",
       "         -0.14774444, -0.30064007,  0.92624954],\n",
       "        [-0.00674861, -0.52885546,  0.41127246, -0.61042182,  0.1044318 ,\n",
       "         -0.97983506, -0.66409302,  0.09843062],\n",
       "        [ 0.36927826,  0.08491174, -0.0572283 , -0.15230798, -0.73807469,\n",
       "         -0.35828861, -0.23048888,  0.5289575 ],\n",
       "        [ 0.17017992, -0.8825674 ,  0.16139561, -0.19445354, -0.33911249,\n",
       "          0.30410292,  0.51387861,  0.46456209],\n",
       "        [-0.41783027, -0.15027064,  0.16518071,  0.4934359 , -0.23920048,\n",
       "         -0.09374193, -0.55281354, -0.59810331],\n",
       "        [ 0.40606731,  0.67676104, -0.03578803,  0.5005843 ,  0.18044961,\n",
       "         -0.32210988,  0.18115685,  0.76920272]]),\n",
       " 'b2': array([[0.77105419],\n",
       "        [0.4937956 ],\n",
       "        [0.52397572],\n",
       "        [0.42532616],\n",
       "        [0.02670987],\n",
       "        [0.10767142]]),\n",
       " 'W3': array([[ 0.04523854, -0.17263197,  0.05159219, -1.14725346, -0.12958876,\n",
       "          0.18699575],\n",
       "        [ 0.85518682, -0.29922345, -0.4661892 , -0.29011448,  0.52997167,\n",
       "          0.19869087],\n",
       "        [-0.30668749,  0.29633509,  0.05588438,  0.55662986, -0.40533054,\n",
       "         -0.19091884],\n",
       "        [-0.22661764, -0.84496075,  0.17083451,  0.15051483,  0.00292659,\n",
       "         -0.13619784]]),\n",
       " 'b3': array([[0.41550696],\n",
       "        [0.22241182],\n",
       "        [0.11951494],\n",
       "        [0.33719002]]),\n",
       " 'W4': array([[-0.11486089,  0.28354174,  1.33304063,  0.12339291],\n",
       "        [ 0.15269459, -0.11009014, -1.35677589, -0.01681931]]),\n",
       " 'b4': array([[0.28386844],\n",
       "        [0.02616072]]),\n",
       " 'W5': array([[0.01282684, 2.46298891]]),\n",
       " 'b5': array([[0.02724705]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (8,2) and (400,2) not aligned: 2 (dim 1) != 400 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4716/2983264398.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4716/4004466259.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, threshold, predict_proba)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \"\"\"\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         A_prob, _ = L_model_forward(X, self.param, \n\u001b[0m\u001b[0;32m    119\u001b[0m                             \u001b[0mactivation_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_activation_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                             \u001b[0mlast_activation_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_forward_activation_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4716/856820474.py\u001b[0m in \u001b[0;36mL_model_forward\u001b[1;34m(X, param, activation_function, last_activation_function)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[0mcaches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# append cache at layer l\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4716/856820474.py\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[1;34m(A_prev, W, b, activation_function)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;36m2.\u001b[0m \u001b[0mcaches\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mcache\u001b[0m \u001b[0mof\u001b[0m \u001b[0mLinear\u001b[0m \u001b[0mforward\u001b[0m \u001b[0mUnit\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mActivation\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \"\"\"\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinear_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mactivation_function\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4716/856820474.py\u001b[0m in \u001b[0;36mlinear_forward\u001b[1;34m(A_prev, W, b)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;36m2.\u001b[0m \u001b[0mcaches\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mcache\u001b[0m \u001b[0mof\u001b[0m \u001b[0mLinear\u001b[0m \u001b[0mforward\u001b[0m \u001b[0mUnit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \"\"\"\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (8,2) and (400,2) not aligned: 2 (dim 1) != 400 (dim 0)"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X)\n",
    "y = Y.values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = y - Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "np.count_nonzero(arr==0) / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model with He initialization\")\n",
    "axes = plt.gca()\n",
    "plot_decision_boundary(lambda x: predict_dec(model.param, x.T), X.values, Y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(pd.DataFrame(np.array([[0,0.7]])))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Evolution model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
