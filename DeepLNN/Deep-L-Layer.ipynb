{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dipple v.0.0.1.ipynb",
      "provenance": [],
      "mount_file_id": "1dQZ8cY7olrKhflK-eV8fO1JqjIPaBsm3",
      "authorship_tag": "ABX9TyOGlic7YZfc3xt+TujffF57",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wallik2/DeepLearningAndrewNG/blob/ch1_NN/DeepLNN/Deep-L-Layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time"
      ],
      "metadata": {
        "id": "mPHVQ83CMBNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Error & Updating\n",
        "\"\"\"\n",
        "def binary_cross_entropy(a,y):\n",
        "  return -((y * np.log(a)) + ((1-y) * np.log(1-a)))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Activation Function\n",
        "\"\"\"\n",
        "def tanh(z):\n",
        "  return ( np.exp(z) - np.exp(-z) ) / ( np.exp(z) + np.exp(-z) )\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "def ReLU(z):\n",
        "  return np.where(z>=0,z,0)\n",
        "\n",
        "def LeakyReLU(z:float):\n",
        "  return np.where(z>=0,z,0.01*z)\n",
        "\n",
        "\"\"\"\n",
        "Derivative of Activation Function wrp. Z\n",
        "\"\"\"\n",
        "def dReLU(z:float):\n",
        "  return np.where(z>=0,1,0)\n",
        "\n",
        "def dLeakyReLU(z:float):\n",
        "  return np.where(z>=0,1,0.01)\n",
        "\n",
        "def dTanh(z:float):\n",
        "  a = tanh(z)  \n",
        "  return 1-a**2\n",
        "\n",
        "\"\"\"\n",
        "For Bi-Deep L layer\n",
        "\"\"\"\n",
        "def thresholder(A,thr):\n",
        "    return np.where(A >= thr , 1, 0)\n"
      ],
      "metadata": {
        "id": "WL3pYvQuME-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Initiate parameter\n",
        "\"\"\"\n",
        "\n",
        "def initiate_param(hyperparam:list):\n",
        "  \"\"\"\n",
        "  Initiate the paramaters W, B for each layer\n",
        "  input : hyperparameter dictionary (specifically, we need the number of unit for every layer)\n",
        "  \"\"\"\n",
        "\n",
        "  np.random.seed(42)\n",
        "  n_unit = hyperparam[\"n_unit\"]\n",
        "  L      = len(n_unit) - 1\n",
        "  param  = dict()\n",
        "\n",
        "  for l in range(L):\n",
        "    param[\"W\" + str(l+1)] = np.random.random(size = (n_unit[l+1],n_unit[l])) * 0.01\n",
        "    param[\"b\" + str(l+1)] = np.random.random(size = (n_unit[l+1],1))\n",
        "  \n",
        "  return param"
      ],
      "metadata": {
        "id": "Sqh2b6lWMFXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Forward Propagation Unit\n",
        "\"\"\"\n",
        "\n",
        "def linear_forward(A_prev,W,b):\n",
        "  Z = np.dot(W,A_prev) + b\n",
        "  linear_cache = (A_prev, W, b)   # A :for dZ, W for dA & to get updating, b for updating , dA for dZ\n",
        "  return Z , linear_cache\n",
        "\n",
        "def linear_activation_forward(A_prev,W,b,activation_function):\n",
        "  Z, linear_cache = linear_forward(A_prev,W,b)\n",
        "\n",
        "  if activation_function == 'sigmoid':\n",
        "    A = sigmoid(Z)\n",
        "  elif activation_function == 'tanh':\n",
        "    A= tanh(Z)\n",
        "  elif activation_function == 'ReLU':\n",
        "    A= ReLU(Z)\n",
        "  elif activation_function == 'LeakyReLU':\n",
        "    A= LeakyReLU(Z)\n",
        "\n",
        "  cache = (Z,linear_cache)    # (Z,A_prev,W,b)\n",
        "\n",
        "  return A,cache\n",
        "\n",
        "\n",
        "def L_model_forward(X,param,activation_function='ReLU'):\n",
        "  \"\"\"\n",
        "  Forward propagation unit from input to output layer \n",
        "  Argument \n",
        "  1. X --- Input denoted as A[0]\n",
        "  2. param --- Weight and Bias of every layer \n",
        "  3. activation_function --- the activation function of hidden layers (default:ReLU)\n",
        "  \n",
        "  Return \n",
        "  1. A --- Output A[L] from the propagation (Z[L] with sigmoid activation function)\n",
        "  2. caches --- the cache of every layer l ; [Z[l] , A[l-1], W[l], b[l]]\n",
        "            --- L elements , each elements (array forms with 4 sub-arrays) contain the cache of its layer\n",
        "                linear_cache : Z[l]\n",
        "                activation_cache : A[l-1], W[l], b[l]\n",
        "  \"\"\"\n",
        "\n",
        "  A = X \n",
        "  L = len(param) // 2    # param stores the weight and bias for L layer, hence len(param) = 2L\n",
        "  \n",
        "\n",
        "  caches = []  \n",
        "\n",
        "  for l in range(L):     # l = 0,1,2,..,L-1\n",
        "    A_prev = A\n",
        "    W =  param[\"W\" + str(l+1)] \n",
        "    b =  param[\"b\" + str(l+1)] \n",
        "    \n",
        "    A,cache =  linear_activation_forward(A_prev, W, b, activation_function) \n",
        "    caches.append(cache)  # append cache at layer l+1\n",
        "\n",
        "  return A, caches"
      ],
      "metadata": {
        "id": "hCAaYiE0MIfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Backward Propagation Unit\n",
        "\"\"\"\n",
        "\n",
        "def linear_backward(dZ,cache):    \n",
        "  \"\"\"\n",
        "  Use dZ from the layer l to obtain dW,dB,dA_prev\n",
        "  Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (Z,(A_prev, W, b)) coming from the forward propagation in the current layer (We use only linear cache anyway)\n",
        "  \n",
        "  Returns:\n",
        "    dA_prev --- Gradient of the cost with respect to the activation node at the previous layer\n",
        "    dW --- Gradient of the cost with the weight in this layer\n",
        "    db --- Gradient of the cost with the bias in this layer\n",
        "  \"\"\"  \n",
        "  _ , activation_cache = cache     # We use only activation cache \n",
        "  ( a_prev,W, _ ) = activation_cache  # We do not use b to update      \n",
        "  m = dZ.shape[1]\n",
        "  \n",
        "  dW = (1/m) * np.dot(dZ,a_prev.T)\n",
        "  db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "\n",
        "  dA_prev = np.dot(W.T,dZ)\n",
        "  \n",
        "  return dA_prev, dW, db  \n",
        "\n",
        "\n",
        "def linear_activation_backward(dA,cache,activation_function):\n",
        "  \"\"\"\n",
        "  Input dA to find dZ, then use dZ to obtain dW,dB,dA_prev\n",
        "  \"\"\"\n",
        "  if activation_function == 'ReLU':\n",
        "    g_ = dReLU\n",
        "  elif activation_function == 'LeakyReLU':\n",
        "    g_ = dLeakyReLU\n",
        "  elif activation_function == 'tanh':\n",
        "    g_ = dTanh\n",
        "  else:\n",
        "    print(f\"The activation function {activation_function} not found, ReLU as default\")\n",
        "    g_ = dReLU\n",
        "\n",
        "  linear_cache, _ = cache  # We use only linear output cache\n",
        "  Z = linear_cache\n",
        "\n",
        "  dZ = dA * g_(Z)\n",
        "  dA_prev, dW, db = linear_backward(dZ,cache)\n",
        "\n",
        "  return dA_prev, dW, db\n",
        "\n",
        "\n",
        "def L_model_backward(A,Y,cache,activation_function='ReLU'):\n",
        "  \"\"\"\n",
        "  Do the whole backward propagation model by retrieving the output from forward propagation model and the actual output\n",
        "  Arguments:\n",
        "  A --- A at the layer L\n",
        "  y --- an actual output\n",
        "  cache --- cache from the forward propagation\n",
        "  activation_function --- activation function for the hidden layer\n",
        "  Return:\n",
        "   grads  -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "  \"\"\"\n",
        "  L = len(cache)                           #cache for each layer\n",
        "  grads = {}\n",
        "  dA = np.divide(1-Y,1-A) - np.divide(Y,A)  # dA_[L] : Input for the first linear activation backward\n",
        "  \n",
        "  for l in reversed(range(L)):\n",
        "\n",
        "    current_cache = cache[l]\n",
        "    ( linear_cache , activation_cache ) = current_cache\n",
        "    Z = linear_cache\n",
        "    a_prev,W,b = activation_cache       # Start with Z_[L] , A_[L-1], W_[L], b_[L]\n",
        "    \n",
        "    dA_prev, dW, db = linear_activation_backward(dA,current_cache,activation_function)\n",
        "    \n",
        "    grads['dW' + str(l+1)] = dW\n",
        "    grads[\"db\" + str(l+1)] = db\n",
        "    \n",
        "    dA = dA_prev\n",
        "\n",
        "  return grads"
      ],
      "metadata": {
        "id": "CrtGzWENMMh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def update_param(param,grads,lr=1e-4):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    1. param -- The current parameter (W1,W2,...,WL,b1,b2,...bL)\n",
        "    2. grads -- the dictionary of gradient that was obtained from L_model_backward function\n",
        "    3. lr (default=1e-4) : Learning rate\n",
        "    Returns:\n",
        "    1. updated_param -- The parameter that got updated\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(param) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    for l in range(L):\n",
        "        param[\"W\" + str(l+1)] = param[\"W\" + str(l+1)] - lr * grads[\"dW\" + str(l + 1)]\n",
        "        param[\"b\" + str(l+1)] = param[\"b\" + str(l+1)] - lr * grads[\"db\" + str(l + 1)]\n",
        "\n",
        "    return param  \n"
      ],
      "metadata": {
        "id": "-6NkNadCMRfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_reporter(A,Y,m,epoch,Epoch):\n",
        "    \"\"\"\n",
        "    report the cost for each epoch\n",
        "    cost function : Binary cross entropy \n",
        "    Arguments:\n",
        "    A --- predicted value from L-Forward model\n",
        "    y --- actual output\n",
        "    m --- total observations \n",
        "    epoch --- current epoch\n",
        "    Epoch --- total epoch\n",
        "    \"\"\"\n",
        "    loss = binary_cross_entropy(A,Y)\n",
        "    cost = np.divide(loss,m)   # No significant difference in speed when compare to '/' though \n",
        "    cost = np.sum(cost,axis=1) \n",
        "    print(f'Epoch {epoch}/{Epoch} : ===Cost=== : {np.squeeze(cost)}')"
      ],
      "metadata": {
        "id": "MZtJnWszMTkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Binary_Deep_L_Layer:\n",
        "  \"\"\"\n",
        "    A Deep neural network with L layers \n",
        "    - Able to fit with the predictors (X) and the response (Y)\n",
        "    - Able to predict_proba and predict with threshold\n",
        "    To see the last fit model parameter, uses self.param where self refer to the fit model\n",
        "  \"\"\"  \n",
        "  def __init__(self, hyperparam:dict ):\n",
        "    \"\"\"\n",
        "    Launch the Deep_L_layer with the given hyperparameter\n",
        "    Arguments:\n",
        "    hyperparam: A dictionary with key:\n",
        "     L --- Number of Layers (Hidden layer(s) + Output layer)\n",
        "     n_unit --- Number of units of that L layer\n",
        "     lr --- Learning rate\n",
        "     forward_activation_function --- Activation function for all hidden layer(s) in forward model (ReLU,LeakyReLU,tanh,sigmoid)\n",
        "     backward_activation_function --- Activation function for all hidden layer(s) in backward model (ReLU,LeakyReLU,tanh,sigmoid)\n",
        "      {\"L\" : 5,\n",
        "      \"n_unit\" : [nrow,8,6,4,2,1],\n",
        "      \"lr\" : 1e-5,\n",
        "      \"forward_activation_function\" : 'tanh' ,\n",
        "      \"backward_activation_function\" : 'ReLU' }\n",
        "    Supported activation\n",
        "    \"\"\"\n",
        "    self.hyperparam = hyperparam #assume include nrow in dict\n",
        "\n",
        "    # Explicit hyperparameter attributes\n",
        "    self.L = hyperparam[\"L\"]\n",
        "    self.lr = hyperparam[\"lr\"]\n",
        "    self.forward_activation_function = hyperparam[\"forward_activation_function\"] \n",
        "    self.backward_activation_function = hyperparam[\"backward_activation_function\"] \n",
        "\n",
        "  def fit(self,X:pd.DataFrame, Y:pd.Series,Epochs:int = 1000,verbose:bool= True,new_fit:bool=True):\n",
        "    \"\"\"\n",
        "    Fit the launched Deep L layer with the given data X , Y\n",
        "    \n",
        "    Arguments:\n",
        "     X --- Pandas Dataframe of predictors\n",
        "     Y --- Pandas Series of response (0 : negative, 1:positive)\n",
        "     Epoch --- number of epochs (default : 1000)\n",
        "     verbose --- report the epochs every 1000 epoch\n",
        "     new_fit --- reset parameters and generate parameters\n",
        "    \"\"\"\n",
        "\n",
        "    ## First, we initiate the attributes\n",
        "\n",
        "    # We turn Dataframe into Numpy format\n",
        "    X = X.to_numpy().T \n",
        "    Y = Y.to_numpy().T \n",
        "    nrow = np.shape(X)[0]\n",
        "\n",
        "    # Assign class attribute\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.m = Y.shape[1]\n",
        "    self.Epochs = Epochs \n",
        "\n",
        "    ## Second, we fit\n",
        "    if new_fit:\n",
        "      self.param = initiate_param(self.hyperparam)\n",
        "\n",
        "    for epoch in range(self.Epochs):\n",
        "      A, cache = L_model_forward(self.X,self.param,\n",
        "                                 activation_function = self.forward_activation_function )\n",
        "\n",
        "      if verbose and epoch % 1000 == 0:\n",
        "        cost_reporter(A,self.Y,self.m,epoch,self.Epochs)\n",
        "\n",
        "      grads = L_model_backward(A,self.Y,cache,self.backward_activation_function)\n",
        "      self.param = update_param(self.param,grads,lr=self.lr)\n",
        "  \n",
        "  def predict_proba(self,X:pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Predict probability of the observation given input X\n",
        "    \n",
        "    Arguments:\n",
        "     X --- Pandas Dataframe or Series of predictors\n",
        "    \"\"\"\n",
        "    X = X.to_numpy().T\n",
        "\n",
        "    A_prob , _ = L_model_forward(X,self.param,activation_function = self.forward_activation_function )\n",
        "\n",
        "    return A_prob\n",
        "\n",
        "  def predict(self,X,threshold:float = 0.5):\n",
        "    \"\"\"\n",
        "    Predict the observation given input X\n",
        "    \n",
        "    Arguments:\n",
        "     X --- Pandas Dataframe or Series of predictors\n",
        "    \"\"\"\n",
        "    A_prob = self.predict_proba(X)\n",
        "    A_pred = thresholder(A_prob,threshold)\n",
        "    return A_pred\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f'Deep_L_Layer({self.hyperparam})'\n",
        "\n",
        "  def __str__(self):\n",
        "    return f'A Deep {self.L} Neural network with learning rate = {self.lr} (Forward activation :{self.forward_activation_function},Backward activation :{self.backward_activation_function})'"
      ],
      "metadata": {
        "id": "_vVbNcrGMV1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "806lCdjxVXKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test case"
      ],
      "metadata": {
        "id": "lNaqbnp7VaTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/dataset/Andrew NG/churn_small3.csv')"
      ],
      "metadata": {
        "id": "PZKVSAK8RN-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparam = {\"L\" : 5,\n",
        "              \"n_unit\" : [3,8,6,4,2,1],\n",
        "              \"lr\" : 1e-5,\n",
        "              \"forward_activation_function\" : 'ReLU',\n",
        "              \"backward_activation_function\" : 'ReLU',\n",
        "              \"keep_prob_sequence\" : [1,0.5,0.6,0.7,1,1]}   #Dropout => No dropout = None OR [1,1,1,1,1,1]\n",
        "\n",
        "X = df[['tenure'\t,'TotalCharges'\t,'PaperlessBilling']]\n",
        "Y = df[['Churn']]\n"
      ],
      "metadata": {
        "id": "huthI9Yu4xtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Binary_Deep_L_Layer(hyperparam)"
      ],
      "metadata": {
        "id": "yakoO0dsNDwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,Y,Epochs=100000)"
      ],
      "metadata": {
        "id": "YYsJGaQRNdb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "70747cae-afe7-49eb-c7a4-3c57cf84b58c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/100000 : ===Cost=== : 1.3092259528752233\n",
            "Epoch 1000/100000 : ===Cost=== : 1.18264973341166\n",
            "Epoch 2000/100000 : ===Cost=== : 1.0948250491187088\n",
            "Epoch 3000/100000 : ===Cost=== : 1.028352257196818\n",
            "Epoch 4000/100000 : ===Cost=== : 0.975404404481697\n",
            "Epoch 5000/100000 : ===Cost=== : 0.9317897592718797\n",
            "Epoch 6000/100000 : ===Cost=== : 0.8950027214519833\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-e0f85bb20ce0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEpochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-688e292e6fe6>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, Epochs, verbose, new_fit)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEpochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m       A, cache = L_model_forward(self.X,self.param,\n\u001b[0;32m---> 65\u001b[0;31m                                  activation_function = self.forward_activation_function )\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-e39de8fde433>\u001b[0m in \u001b[0;36mL_model_forward\u001b[0;34m(X, param, activation_function)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mlinear_activation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mcaches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# append cache at layer l+1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-e39de8fde433>\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[0;34m(A_prev, W, b, activation_function)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mactivation_function\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-e39de8fde433>\u001b[0m in \u001b[0;36mlinear_forward\u001b[0;34m(A_prev, W, b)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mlinear_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# A :for dZ, W for dA & to get updating, b for updating , dA for dZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlinear_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}